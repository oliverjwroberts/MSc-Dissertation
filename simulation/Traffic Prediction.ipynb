{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a3713a-3586-4779-a21d-68b3fad4a908",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Data\n",
    "First, all necessary libraries are imported along with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a8dc6-8cf6-4c8b-adb7-bb75ac403231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import datetime\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neural_network\n",
    "import sklearn.ensemble\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check SUMO_HOME is set\n",
    "import os, sys\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "    \n",
    "import traci\n",
    "import sumolib\n",
    "import altair as alt\n",
    "import folium\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1816bd-7ac3-4795-9273-9b5e90f2ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'count_point_id': 'string',\n",
    "    'direction_of_travel': 'string',\n",
    "    'count_date': 'string',\n",
    "    'hour': 'string',\n",
    "    'road_name': 'string',\n",
    "    'road_type': 'string',\n",
    "    'latitude': 'float',\n",
    "    'longitude': 'float',\n",
    "    'link_length_km': 'float',\n",
    "    'pedal_cycles': 'int',\n",
    "    'two_wheeled_motor_vehicles': 'int',\n",
    "    'cars_and_taxis': 'int',\n",
    "    'buses_and_coaches': 'int',\n",
    "    'lgvs': 'int',\n",
    "    'all_hgvs': 'int',\n",
    "    'all_motor_vehicles': 'int' \n",
    "}\n",
    "\n",
    "cols = list(col_types.keys())\n",
    "\n",
    "dft_counts = pd.read_csv('dft_count_swansea.csv', sep=',', header=0,\n",
    "                         index_col=None, dtype=col_types, usecols=cols, na_values='')\n",
    "dft_counts['count_date'] = pd.to_datetime(dft_counts['count_date'], format= '%Y-%m-%d')\n",
    "dft_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2739241-1a64-4f34-9de6-8e793824d038",
   "metadata": {},
   "source": [
    "# 2. Cleaning the Data\n",
    "In this section:\n",
    "- Additional features are imputed into the dataset.\n",
    "- Date values are split into day, month and year. \n",
    "- The hourly count data is unstacked and reshaped.\n",
    "- Categorical features are encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbe951-9cbc-426c-a5d9-08ca712000a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edge_speed(coordinates, net, radius=150):    \n",
    "    # coordinates must be latitude then longitude\n",
    "    x, y = net.convertLonLat2XY(coordinates[1], coordinates[0]) # find x/y positions within network\n",
    "    edges = net.getNeighboringEdges(x, y, radius) # create list of edges in radius of point\n",
    "    \n",
    "    if len(edges) == 0:\n",
    "        raise ValueError(f'There were no edges found within the {radius} radius of {coordinates}')\n",
    "    \n",
    "    # pick closest edge according to distance\n",
    "    try:\n",
    "        distancesAndEdges = sorted([(dist, edge) for edge, dist in edges], key=lambda x: x[0])\n",
    "    except: \n",
    "        # if an edge has two directions and thus each direction has same distance      \n",
    "        raise ValueError('Multiple closest edges')\n",
    "        # can use the following to get a list of all same occurences\n",
    "        # sorted_edges = sorted([(dist, edge) for edge, dist in edges], key=lambda x: x[0])\n",
    "        # occ = [edge_tuple for edge_tuple in sorted_edges if edge_tuple[0] == sorted_edges[0][0]]\n",
    "        \n",
    "    dist, closestEdge = distancesAndEdges[0]\n",
    "    return closestEdge._speed\n",
    "\n",
    "\n",
    "def get_cp_speeds(dft_counts, net):\n",
    "    start = time.time()\n",
    "    \n",
    "    # extract unique count points and compute edge speeds \n",
    "    count_points = dft_counts.groupby('count_point_id').first().reset_index()\n",
    "    cols_to_keep = ['count_point_id', 'road_name', 'road_type',\n",
    "                    'latitude', 'longitude', 'link_length_km']\n",
    "    count_points = count_points[cols_to_keep]\n",
    "    edge_speeds = [find_edge_speed((lat, long),net) for lat, long in\n",
    "                   zip(count_points['latitude'], count_points['longitude'])]\n",
    "    \n",
    "    MPS_TO_KPH = 3.6 # MPS to KPH\n",
    "    \n",
    "    # take 5 mph off max road speed to better represent avg speed of vehicles\n",
    "    # count_points['avg_speed'] = [speed - (5/MPH_TO_MPS) for speed in edge_speeds]\n",
    "    count_points['avg_speed'] = [speed for speed in edge_speeds]\n",
    "    \n",
    "    print(f'Got count point speeds in {np.round(time.time()-start,2)} seconds')\n",
    "    return count_points\n",
    "\n",
    "net = sumolib.net.readNet('osm.net.xml') # read net file\n",
    "count_points = get_cp_speeds(dft_counts, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1439051-63c1-4787-bf0a-4470488f1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_speeds(dft_counts, count_points):\n",
    "    start = time.time()\n",
    "    dft_counts = pd.merge(dft_counts,\n",
    "                          count_points[['count_point_id','avg_speed']],\n",
    "                          on ='count_point_id', \n",
    "                          how ='inner')\n",
    "    print(f'Imputed speeds in {np.round(time.time()-start,2)} seconds')\n",
    "    return dft_counts\n",
    "\n",
    "dft_counts = impute_speeds(dft_counts, count_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a57b4e-0099-4d81-a244-81b227efdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(df):\n",
    "    # unstack hourly counts\n",
    "    clean_df = df.set_index(['count_point_id','direction_of_travel','count_date','hour'])\n",
    "    clean_df = clean_df[['all_motor_vehicles']]\n",
    "    clean_df = clean_df.unstack(level=-1)\n",
    "    clean_df.reset_index(inplace=True)\n",
    "    \n",
    "    # flatten multi-level col index and rename\n",
    "    clean_df.columns = clean_df.columns.to_flat_index()\n",
    "    col_names = [a for a in clean_df.columns]\n",
    "    col_names = [name[0] if col_names.index(name) <= 2 else name[1] for name in col_names]\n",
    "    clean_df.columns = col_names\n",
    "    ordered_col_names = ['count_point_id','direction_of_travel','count_date',\n",
    "                         '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "    clean_df = clean_df[ordered_col_names]\n",
    "    return clean_df\n",
    "\n",
    "dft_counts_clean = clean_cols(dft_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0623c84-0097-4e7e-a21a-bbba9f7807c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cp_date_ranges(df):\n",
    "    cps = [cp_id for cp_id in pd.unique(df['count_point_id'])]\n",
    "    min_date = []\n",
    "    max_date = []\n",
    "    for cp_id in cps:\n",
    "        min_date.append(df.loc[df['count_point_id'] == cp_id]['count_date'].min())\n",
    "        max_date.append(df.loc[df['count_point_id'] == cp_id]['count_date'].max())\n",
    "    cp_date_ranges = pd.DataFrame(\n",
    "        {\n",
    "            'cp_id': cps,\n",
    "            'min_date': min_date,\n",
    "            'max_date': max_date\n",
    "        })\n",
    "    return cp_date_ranges\n",
    "\n",
    "\n",
    "def get_cp_dates(df):\n",
    "    cps = [cp_id for cp_id in pd.unique(df['count_point_id'])]\n",
    "    cps_longform = []\n",
    "    dates = []\n",
    "    for cp_id in cps:\n",
    "        filtered_df = df.loc[df['count_point_id'] == cp_id]\n",
    "        cps_longform = cps_longform + [i for i in df.loc[df['count_point_id'] == cp_id]['count_point_id']]\n",
    "        dates = dates + [i for i in df.loc[df['count_point_id'] == cp_id]['count_date']]\n",
    "    cp_dates = pd.DataFrame(\n",
    "        {\n",
    "            'cp_id': cps_longform,\n",
    "            'dates': dates\n",
    "        })\n",
    "    return cp_dates\n",
    "\n",
    "cp_date_ranges = get_cp_date_ranges(dft_counts_clean)\n",
    "cp_dates = get_cp_dates(dft_counts_clean)\n",
    "\n",
    "# Visualise\n",
    "selector = alt.selection_interval(encodings=['y'])\n",
    "\n",
    "dates_range = alt.Chart(cp_date_ranges).mark_bar().encode(\n",
    "    x=alt.X('cp_id:O', title='Count Point ID'),\n",
    "    y=alt.Y('min_date', title='Minimum - Maximum Date'),\n",
    "    y2=alt.Y2('max_date', title=None),\n",
    "    tooltip=[ 'cp_id','min_date', 'max_date']\n",
    ").properties(\n",
    "    title = 'Count Point Active Dates',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "dates = alt.Chart(cp_dates).mark_circle(color='orange').encode(\n",
    "    x=alt.X('cp_id:O'),\n",
    "    y=alt.Y('dates'),\n",
    "    tooltip=[ 'cp_id','dates']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ").add_selection(\n",
    "    selector\n",
    ")\n",
    "\n",
    "dates_range + dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f29ad-9396-4f91-ba2f-2151cc3f6e8e",
   "metadata": {},
   "source": [
    "# 3. Predictive Models\n",
    "## 3.1 Random Forest\n",
    "In this section, we will use an Sklearn Random Forest (RF) Regressor to predict the 18th hour traffic counts for each unique count point and direction of travel. This is similar to the approach in [this paper](https://ieeexplore.ieee.org/abstract/document/9230762?casa_token=LawxRQvGmnwAAAAA:rNcIU4KH2tkbI9x6f4ZIQfzioCoE7dgoAQCbcpUnCpYLl8h3p_md8eYN7FtIMtQe8Zbiz9brh3U), however they used a CNN-LTSM. By using an RF, this method does not really capture the temporal aspect of the dataset, i.e. the year the counting took place. In further sections we will explore more sophisticated and up-to-date predictive methods; ones that can be used to predict the next year traffic counts based on the previous 5 years. However, this will require various different data manipulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423098c6-727a-4274-a8c5-c3f869022484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group counts by cp id, year, and direction of travel and compute the mean for each group\n",
    "grouped_counts = dft_counts_clean.groupby(['count_point_id',dft_counts_clean['count_date'].dt.year,'direction_of_travel']).mean().reset_index()\n",
    "# add speed and road type to the grouped data according to cp id\n",
    "grouped_counts = pd.merge(grouped_counts,\n",
    "                          count_points[['count_point_id','avg_speed','road_type']], # avg speed may not be useful\n",
    "                          on ='count_point_id', \n",
    "                          how ='inner')\n",
    "# one hot encode direction of travel and road type\n",
    "counts_for_model = pd.get_dummies(grouped_counts, prefix=['direction_of_travel', 'road_type'], columns=['direction_of_travel', 'road_type'])\n",
    "counts_for_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c2754-39f4-44a6-a8e0-615d9149c836",
   "metadata": {},
   "source": [
    "The following function can be used to count how many years each unique count point is active, and record which years.\n",
    "\n",
    "```python\n",
    "def how_many_years(df):\n",
    "    '''\n",
    "    Function to count how many years each unique count point is active, and record which years.\n",
    "    '''\n",
    "    how_many_years = {}\n",
    "    \n",
    "    for cp in [cp_id for cp_id in pd.unique(df['count_point_id'])]:\n",
    "        years = [year for year in pd.unique(df.loc[df['count_point_id']==cp]['count_date'])]\n",
    "        count = len(years)\n",
    "        how_many_years[cp] = (count, years)\n",
    "    return how_many_years\n",
    "\n",
    "how_many_years(grouped_counts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97368e3-0fba-406c-a530-fa74a185c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix numpy seed\n",
    "SEED = 202\n",
    "np.random.seed(SEED) \n",
    "\n",
    "# split data into train and test sets\n",
    "x = counts_for_model.drop('18', axis=1)\n",
    "y = counts_for_model['18']\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717f2f6-1397-421b-a9f2-a2ff00a7a685",
   "metadata": {},
   "source": [
    "```python\n",
    "# list models to be optimised\n",
    "rf = sklearn.ensemble.RandomForestRegressor()\n",
    "\n",
    "# list parameters to search in dicts\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'criterion': ['mse','mae'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "model_list = [rf]\n",
    "model_names = ['RF']\n",
    "model_params = [rf_params]\n",
    "\n",
    "def optimise_hyperparameters(model_list, model_params, x_train=x_train, y_train=y_train):\n",
    "    grid_search_list = []\n",
    "    for i in range(len(model_list)):\n",
    "        start = time.time()\n",
    "        model, params = model_list[i], model_params[i]\n",
    "        grid_search = sklearn.model_selection.GridSearchCV(model, params, n_jobs=5, cv=5)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        grid_search_list.append(grid_search)\n",
    "        print(f'Found optimal hyperparameter combination for {model} in {np.round(time.time()-start,4)} seconds')\n",
    "        print(grid_search.best_estimator_)\n",
    "        print(grid_search.best_score_)\n",
    "    return grid_search_list\n",
    "\n",
    "grid_search_list = optimise_hyperparameters(model_list, model_params)\n",
    "\n",
    "Found optimal hyperparameter combination for RandomForestRegressor() in 308.3771 seconds\n",
    "RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "0.9809518061349201\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81745a5-7946-42ae-896a-ded9d68ee5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the models and their hyperparameters\n",
    "rf = sklearn.ensemble.RandomForestRegressor(n_estimators=200, criterion='mse', max_depth=10, \n",
    "                                            max_features='sqrt', random_state=22)\n",
    "\n",
    "# bundle in a list to pass to function\n",
    "model_list = [rf]\n",
    "model_names = ['RF']\n",
    "\n",
    "def train_models(model_list, model_names, \n",
    "                 x_train=x_train, x_test=x_test,\n",
    "                 y_train=y_train, y_test=y_test):\n",
    "    '''\n",
    "    Function that takes a list of models, and the x and y data.\n",
    "    It fits each model and calls the accuracy function to compute a number of statistical measures. \n",
    "    Finally, a DataFrame with all the data is produced.\n",
    "\n",
    "    '''\n",
    "    accuracy_list = []\n",
    "    for i in range(len(model_list)):\n",
    "        model, model_name = model_list[i], model_names[i] # store the model object and name\n",
    "        start = time.time() # start timer\n",
    "        model.fit(x_train, y_train)# fit model to training data\n",
    "        lap = time.time()-start\n",
    "        print(f'Trained {model_name} in {np.round(lap,4)} seconds')\n",
    "        accuracy_list.append(accuracy(model, model_name, x_train, y_train, lap)) # train measures\n",
    "        accuracy_list.append(accuracy(model, model_name, x_test, y_test, lap)) # test measures                  \n",
    "    accuracy_table = pd.DataFrame(accuracy_list) # make table from list of dicts\n",
    "    return accuracy_table\n",
    "        \n",
    "        \n",
    "def accuracy(model, model_name, x, y, train_time):\n",
    "    '''\n",
    "    Function that takes a model, and x and y data,\n",
    "    and computes a number of statistical measures.\n",
    "    \n",
    "    '''\n",
    "    predictions = model.predict(x)\n",
    "    predictions = [np.float64(item) for item in predictions]\n",
    "    measures = {\n",
    "        'Model': model_name,\n",
    "        'Train Time': train_time,\n",
    "        'MSE': np.mean((y-predictions)**2),\n",
    "        'RMSE': np.sqrt(np.mean((y-predictions)**2)),\n",
    "        'MAE': np.mean(abs(y-predictions)),\n",
    "        'MAPE': 100 - np.mean(100*(abs(y-predictions)/y)),\n",
    "        'R2': model.score(x, y)}\n",
    "    return measures\n",
    "\n",
    "measures = train_models(model_list, model_names)\n",
    "measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d305516-b19f-43d0-a575-bc7a67bbb4f0",
   "metadata": {},
   "source": [
    "The measures DataFrame returns an infinity becuase `y_test.iloc[189] == 0`. When calculating the MAPE you divide by the actual value which in this case is zero, thus, inf is returned. A great explanation of shortcomings of MAPE can be found [here](https://stats.stackexchange.com/a/299713).\n",
    "\n",
    "Also, **please be cautious of which accuracy measures you are reporting!** If you were to report the prediction accuracy of the model when predicting the traffic counts of the entire dataset. These accuracy values are going to be abnormally high due to the fact the model was trained, and has seen, 80% of the data already. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a48208-adb9-400e-87ee-53ad76ebef89",
   "metadata": {},
   "source": [
    "The following function can be used to un-dummify data passed to the model.\\\n",
    "However, since the seperation of grouped_counts and counts_for_model, this has become obsolete.\n",
    "\n",
    "```python\n",
    "def undummify(df, prefix_sep=\"_\"):\n",
    "    '''\n",
    "    Function collapses a \"dummified\" dataframe while keeping the order of columns.\n",
    "    Taken from: https://stackoverflow.com/a/62085741/13937247\n",
    "    '''\n",
    "    keys = [item.split(prefix_sep)[:-1] for item in df.columns]\n",
    "    cols2collapse = {}\n",
    "    for i in range(len(keys)):\n",
    "        if len(keys[i]) > 1:\n",
    "            key = '_'.join(keys[i])\n",
    "            cols2collapse[key] = '_' in df.columns[i]\n",
    "    series_list = []\n",
    "    for col, needs_to_collapse in cols2collapse.items():\n",
    "        if needs_to_collapse:\n",
    "            undummified = (\n",
    "                df.filter(like=col)\n",
    "                .idxmax(axis=1)\n",
    "                .apply(lambda x: x.split(prefix_sep, maxsplit=-1)[-1])\n",
    "                .rename(col)\n",
    "            )\n",
    "            series_list.append(undummified)\n",
    "        else:\n",
    "            series_list.append(df[col])\n",
    "    undummified_df = pd.concat(series_list, axis=1)\n",
    "    return undummified_df\n",
    "\n",
    "# extract and convert dummied columns\n",
    "dummied_cols = ['direction_of_travel_E','direction_of_travel_N',\n",
    "                'direction_of_travel_S','direction_of_travel_W',\n",
    "                'road_type_Major','road_type_Minor']\n",
    "dummied_df = counts_for_model[dummied_cols]\n",
    "undummied_df = undummify(dummied_df)\n",
    "\n",
    "# replace dummied columns with undummied\n",
    "grouped_counts.drop(columns=dummied_cols, inplace=True)\n",
    "grouped_counts = grouped_counts.join(undummied_df)\n",
    "grouped_counts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17aaa8c-5c9d-4c42-93f9-6917bca71be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict counts for each count point/direction of travel, for each year they were active. \n",
    "y_pred = rf.predict(x)\n",
    "results = grouped_counts.copy()\n",
    "results['18_pred'] = np.round(y_pred).astype(int) # round to nearest int, and convert from float -> int\n",
    "results.drop(columns='18', inplace=True)\n",
    "\n",
    "def generate_data_for_sumo(grouped_df):\n",
    "    '''\n",
    "    Function to aggregate data into form that SUMO wants to create traffic demand.\n",
    "    At the moment, this function takes the average traffic count across all years for each count point and direction. \n",
    "    '''\n",
    "    cols_to_keep = ['count_point_id','direction_of_travel','avg_speed','18_pred']\n",
    "    df_for_sumo = grouped_df[cols_to_keep]\n",
    "    df_for_sumo = df_for_sumo.groupby(['count_point_id', 'direction_of_travel']).mean().reset_index()\n",
    "    df_for_sumo['18_pred'] = np.round(df_for_sumo['18_pred']).astype(int)\n",
    "    return df_for_sumo\n",
    "\n",
    "sumo_counts = generate_data_for_sumo(results)\n",
    "sumo_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12c344-621a-4818-bc90-5fcf76a7caa5",
   "metadata": {},
   "source": [
    "## 3.2 CNN\n",
    "In this section we use a CNN to predict the 18th hour traffic counts after being exposed to the *m* \\* *n* matrix of data as used above; where *m* is the directional count points and *n* is the hourly counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b9578-e48d-490e-ac8e-288c2fb3c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow model here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56c5e0-beeb-476a-9523-80e6b8aae325",
   "metadata": {},
   "source": [
    "# 4. Traffic Generation\n",
    "In this we generate traffic demand using SUMO's [routes from observation points](https://sumo.dlr.de/docs/Demand/Routes_from_Observation_Points.html) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b134af6-164e-46cb-af11-846329ff59b9",
   "metadata": {},
   "source": [
    "## 4.1 Flowrouter.py\n",
    "SUMO's [flowrouter.py](https://sumo.dlr.de/docs/Tools/Detector.html#flowrouterpy) serves as a drop in replacement for dfrouter which calculates a set of routes **-o** and traffic flows **-e** from given detectors **-d** and their measurements **-f** on a given network (option **-n**). Flowrouter works by solving a maximum flow problem in the given network assuming the measured flows as capacity. The input data is by default aggregated over the whole file but can be split into intervals by setting **-i**. Example call for hourly aggregation:\n",
    "`<SUMO_HOME>/tools/detector/flowrouter.py -n input_net.net.xml -d detectors.xml -f flows20140520.csv -o routes.xml -e flows.xml -i 60`\n",
    "Detectors which have no data (in the specified interval) or are permanently zero are ignored by default. To include them into the calculations use **--respect-zero**.\n",
    "\n",
    "The flowrouter requires the following input files:\n",
    "The traffic flow file is described in the corresponding [dfrouter documentation](https://sumo.dlr.de/docs/Demand/Routes_from_Observation_Points.html#computing_flows).\n",
    "\n",
    "As detector file input you can either use:\n",
    "- The detector file as described in the [dfrouter documentation](https://sumo.dlr.de/docs/Demand/Routes_from_Observation_Points.html#computing_detector_types)\n",
    "- A detector file with types as [generated by dfrouter](https://sumo.dlr.de/docs/Demand/Routes_from_Observation_Points.html#computing_detector_types)\n",
    "- A file with [induction loop definitions](https://sumo.dlr.de/docs/Simulation/Output/Induction_Loops_Detectors_%28E1%29.html)\n",
    "\n",
    "When loading a detector file without `type` information or setting the option **--revalidate-detectors**, all network edges will be re-classified as sources, sinks or in-between.\n",
    "- any edge without incoming edges will be marked as a source\n",
    "- any edge without outgoing edges will be marked as a sink\n",
    "- any edge that is neither source or sink is in-between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8e811-a57b-4673-9f50-9e6b42c574cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_edges(coordinates, net, radius=150, edges_to_return=2):\n",
    "    '''\n",
    "    Function to find closest n edges to specified coordinates.\n",
    "    '''\n",
    "    # coordinates must be tuple of latitude then longitude\n",
    "    x, y = net.convertLonLat2XY(coordinates[1], coordinates[0]) # find x/y positions within network\n",
    "    edges = net.getNeighboringEdges(x, y, radius) # create list of edges in radius of point\n",
    "    \n",
    "    if len(edges) == 0:\n",
    "        raise ValueError(f'There were no edges found within the {radius} radius of {coordinates}')\n",
    "    \n",
    "    # find indices of n closest edges according to distance\n",
    "    distances = np.array([info[1] for info in edges])\n",
    "    min_dist_indices = np.argsort(distances)[:edges_to_return]\n",
    "    closest_edges = np.array(edges)[min_dist_indices] \n",
    "    closest_edges = [edge[0] for edge in closest_edges]\n",
    "    \n",
    "    return closest_edges\n",
    "\n",
    "count_points['closest_edges'] = [find_closest_edges((lat, long), net, edges_to_return=2)\n",
    "                                 for lat, long in zip(count_points['latitude'], count_points['longitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49799496-cdab-4854-8fd5-25f30a2a6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_direction(edge):\n",
    "    '''\n",
    "    Function to determine which direction a specific edge is going. \n",
    "    '''\n",
    "    from_x, from_y, from_z = edge._from._coord\n",
    "    to_x, to_y, to_z = edge._to._coord\n",
    "    dx = to_x - from_x\n",
    "    dy = to_y - from_y\n",
    "    if abs(dx) > abs(dy): # then edge direction is east or west\n",
    "        if to_x > from_x: \n",
    "            direction = 'E'\n",
    "        else: \n",
    "            direction = 'W'\n",
    "    if abs(dx) < abs(dy): # then edge direction is north or south\n",
    "        if to_y > from_y:\n",
    "            direction = 'N'\n",
    "        else:\n",
    "            direction = 'S'\n",
    "    return direction\n",
    "    \n",
    "\n",
    "def match_edges_directions(sumo_counts, count_points):\n",
    "    '''\n",
    "    Function to match edges with count point directions.\n",
    "    In some cases, count points have more than two directions when they are located near junctions/roundabouts;\n",
    "    here, the edge is set to NaN, and is dropped for simplicity.\n",
    "    Note: only two edges are retrieved in the above cell\n",
    "    '''\n",
    "    start = time.time()\n",
    "    \n",
    "    edge_col_list = []\n",
    "    for cp_id in count_points['count_point_id']:\n",
    "        cp_actual_dirs = [actual_dir for actual_dir in\n",
    "                          sumo_counts.loc[sumo_counts['count_point_id']==cp_id]['direction_of_travel']]\n",
    "        edges = count_points.loc[count_points['count_point_id']==cp_id]['closest_edges'].iloc[0]\n",
    "        edge_dirs = [get_edge_direction(edge) for edge in edges]\n",
    "        for direction in cp_actual_dirs:\n",
    "            if direction in edge_dirs:\n",
    "                idx = edge_dirs.index(direction)\n",
    "                cp_actual_dirs[cp_actual_dirs.index(direction)] = edges[idx]\n",
    "            else: \n",
    "                cp_actual_dirs[cp_actual_dirs.index(direction)] = np.nan\n",
    "        edge_col_list = edge_col_list + cp_actual_dirs\n",
    "    sumo_counts['edge'] = edge_col_list\n",
    "    sumo_counts.dropna(axis=0, inplace=True)\n",
    "    sumo_counts.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f'Matched edges and directions in {np.round(time.time()-start, 2)} seconds')\n",
    "    return sumo_counts\n",
    "\n",
    "\n",
    "sumo_counts = match_edges_directions(sumo_counts, count_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd1e4c-f837-4bed-adbb-3c919470ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_detector_type(df, cp_df, how='polygon'):\n",
    "    '''\n",
    "    Function to determine wether a count point will be classed as a source, sink or inbetween.\n",
    "    This function can decide how to class the count points via several methods: 'polygon' or 'random'.\n",
    "    'polygon' -> points outside the polygon, i.e., count points close to the Swansea council boundary are either sources or sinks;\n",
    "    others within the polygon are inbetween. \n",
    "    'random' -> all points are randomly assigned source, sink, or inbetween. \n",
    "    '''\n",
    "    detector_types = []\n",
    "    \n",
    "    if how == 'polygon':\n",
    "        polygon_df = gpd.read_file('odpolygon.json')       \n",
    "        polygon = polygon_df.geometry.unary_union # return a geometry containing the union of all geometries in the GeoSeries.\n",
    "        # transform longitude and latitude into a list of shapely.Point objects\n",
    "        points_df = gpd.GeoDataFrame(count_points, geometry=gpd.points_from_xy(count_points.longitude, count_points.latitude))\n",
    "        in_polygon = points_df.geometry.within(polygon).to_list()\n",
    "        for idx, boolean in enumerate(in_polygon):\n",
    "            if boolean:\n",
    "                detector_types.append('between')\n",
    "            else:\n",
    "                detector_types.append(np.random.choice(['source','sink']))  \n",
    "        \n",
    "    if how == 'random':\n",
    "        for i in range(cp_df.shape[0]):\n",
    "            detector_types.append(np.random.choice(['source','sink', 'between']))\n",
    "            \n",
    "    cp_df['detector_type'] = detector_types\n",
    "    merged_df = pd.merge(df, cp_df[['count_point_id','detector_type']], on='count_point_id', how='inner')        \n",
    "    return merged_df, cp_df\n",
    "\n",
    "flowrouter_data, count_points = impute_detector_type(sumo_counts, count_points, how='random')\n",
    "flowrouter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d13f86-d6b6-4afa-86d8-957496c9a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = gpd.read_file('odpolygon.json')\n",
    "swansea_coords = [51.6195955, -3.9459248]\n",
    "m = folium.Map(location=swansea_coords, zoom_start=11)\n",
    "folium.GeoJson(polygon, name='poly').add_to(m)\n",
    "for index, row in count_points.iterrows():\n",
    "    if row.detector_type == 'between':\n",
    "        colour = 'black'\n",
    "    elif row.detector_type == 'source':\n",
    "        colour = 'green'\n",
    "    elif row.detector_type == 'sink':\n",
    "        colour = 'red'\n",
    "    folium.Marker((row.latitude, row.longitude), popup = row.count_point_id, icon=folium.Icon(color=colour)).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f5f7d-be67-4107-93ee-d1c7ccc27dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detectors_file(df):\n",
    "    '''\n",
    "    Function to generate the SUMO detector file from the sumo_count df.\n",
    "    '''\n",
    "    with open(\"detectors.xml\", 'w') as outf:\n",
    "        outf.write(\"<detectors>\\n\")\n",
    "        for index, row in df.iterrows():\n",
    "            out_string = (f\"    <detectorDefinition id='{row.count_point_id+'_'+row.direction_of_travel}' lane='{row.edge._lanes[0].getID()}' pos='{row.edge._lanes[0]._length/2}' type='{row.detector_type}'/>\\n\")\n",
    "            outf.write(out_string.replace(\"'\",'\"'))\n",
    "        outf.write(\"</detectors>\")\n",
    "        \n",
    "\n",
    "def generate_detector_flows_file(df):\n",
    "    '''\n",
    "    Function to generate the SUMO detector flow file from the sumo_count df.\n",
    "    Note: Speeds need to be in kph!\n",
    "    '''\n",
    "    begin_time = 0\n",
    "    with open(\"detector_flows.csv\", 'w') as outf:\n",
    "        outf.write(\"Detector;Time;qPKW;vPKW\\n\")\n",
    "        for index, row in df.iterrows():\n",
    "            out_string = (f\"{row.count_point_id+'_'+row.direction_of_travel};{begin_time};{row['18_pred']};{np.round(row.avg_speed,4)}\\n\")\n",
    "            outf.write(out_string)\n",
    "\n",
    "\n",
    "generate_detectors_file(flowrouter_data)\n",
    "generate_detector_flows_file(flowrouter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf3287-5935-40fd-858e-1bd06a6f170f",
   "metadata": {},
   "source": [
    "Apparently you can run CMD commands [straight from a Jupyter Notebook cell](https://anaconda.zendesk.com/hc/en-us/articles/360023858254-Executing-Terminal-Commands-in-Jupyter-Notebooks). The following commands use SUMO's flowrouter.py script to generate traffic from the detector files we have just made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a443b-6060-4015-a44b-02472fee9580",
   "metadata": {},
   "source": [
    "```\n",
    "!cd C:\\Users\\ollir\\OneDrive\\Documents\\University\\Data Science MSc\\Dissertation\\To-Fly-or-Not-to-Fly\\simulation\n",
    "!flowrouter.py -n osm.net.xml -d detectors.xml -f detector_flows.csv -o traffic_routes.xml -e traffic_flows.xml -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b9c25-963b-4e45-86f2-42ede76b0c96",
   "metadata": {},
   "source": [
    "Output from using 'polygon' to classify sources and sinks.\n",
    "```\n",
    "Reading net\n",
    "35388 edges read\n",
    "Reading detectors\n",
    "Loaded 33 sources and 39 sinks from detector file. Added 0 sources and 0 sinks from the network\n",
    "Reading flows\n",
    "Calculating routes\n",
    "33 sources, 0 unlimited\n",
    "39 sinks, 0 unlimited\n",
    "Writing 4886 vehicles from 25 sources between time 0 and 60 (minutes)\n",
    "  unused sources: -24540305#0 -60187252#5 -60861135#10 25384719#1 25744946#0 60861135#6 65331896#2 7817485#0\n",
    "```\n",
    "\n",
    "Output from using 'random' to classify sources and sinks.\n",
    "```\n",
    "Reading net\n",
    "35388 edges read\n",
    "Reading detectors\n",
    "Loaded 63 sources and 68 sinks from detector file. Added 0 sources and 0 sinks from the network\n",
    "Reading flows\n",
    "Calculating routes\n",
    "63 sources, 0 unlimited\n",
    "68 sinks, 0 unlimited\n",
    "Writing 11524 vehicles from 39 sources between time 0 and 60 (minutes)\n",
    "  unused sources: -137918290#1 -137918296#2 -139239682#3 -15522127#2 -22564929 -22963191 -23494024#2 -40301262#0 -50249435 -69906441#0 -939464283 -96897376#4 10160480#0 10754466#1 10754555#0 13449572#0 13451018#0 137918296#1 15522127#2 25384719#1 26584579#0 40301262#0 50249435 96897366\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92889760-40bc-401a-88dd-0a1cfee715a9",
   "metadata": {},
   "source": [
    "## 4.2 RandomTrips.py & Calibrators\n",
    "Another solution to translating traffic counts to traffic demand in SUMO is by using calibrators. First, a set of random routes need to be generated, then claibrators are used to adjust the vehicles flow on counter positions, so the overall number is approximately close to the real readings.\n",
    "\n",
    "**Note: When using --validate, trip files are generated which results in the simulation running very slowly.** Just use route files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec015c9b-3f15-497b-99be-40e2103b340b",
   "metadata": {},
   "source": [
    "```\n",
    "!cd C:\\Users\\ollir\\OneDrive\\Documents\\University\\Data Science MSc\\Dissertation\\To-Fly-or-Not-to-Fly\\simulation\n",
    "!randomTrips.py -n osm.net.xml -o traffic_trips.xml --route-file traffic_routes.xml -e 3600 -p 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad043e-9bcb-4ef3-8f2a-ad4a05fd2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_calibrator_file(df):\n",
    "    '''\n",
    "    Function to generate the SUMO detector file from the sumo_count df.\n",
    "    '''\n",
    "    FREQ = 1 # the aggregation interval in which to calibrate the flows. default is step-length\n",
    "    \n",
    "    with open(\"calibrator.xml\", 'w') as outf:\n",
    "        outf.write(\"<additional>\\n\")\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            out_string = (f\"    <routeProbe id='{row.count_point_id+'_'+row.direction_of_travel+'_probe'}' edge='{row.edge._id}' freq='{FREQ}' file='route_probe_output.xml'/>\\n\")\n",
    "            outf.write(out_string.replace(\"'\",'\"'))\n",
    "            \n",
    "        for index, row in df.iterrows():\n",
    "            out_string = (f\"    <calibrator id='{row.count_point_id+'_'+row.direction_of_travel}' edge='{row.edge._id}' pos='{row.edge._lanes[0]._length/2}' freq='{FREQ}' routeProbe='{row.count_point_id+'_'+row.direction_of_travel+'_probe'}' output='calibrator_output.xml'>\\n\")\n",
    "            outf.write(out_string.replace(\"'\",'\"'))\n",
    "            out_string = (f\"        <route id='{'fallback_'+row.count_point_id+'_'+row.direction_of_travel}' edges='{row.edge._id}'/>\\n\")\n",
    "            outf.write(out_string.replace(\"'\",'\"'))\n",
    "            out_string = (f\"        <flow  begin='0' end='3600' route='{'fallback_'+row.count_point_id+'_'+row.direction_of_travel}' vehsPerHour='{row['18_pred']}' speed='{row.avg_speed}'/>\\n\")\n",
    "            outf.write(out_string.replace(\"'\",'\"'))\n",
    "            outf.write(\"    </calibrator>\\n\")\n",
    "        \n",
    "        # generate edge-based dump\n",
    "        out_string = (\"    <edgeData id='measure_1' file='edge_output.xml'/>\\n\")\n",
    "        outf.write(out_string.replace(\"'\",'\"'))        \n",
    "        \n",
    "        outf.write(\"</additional>\")\n",
    "\n",
    "'''\n",
    "note:  position of calibrator is actually ignored at the moment. https://github.com/eclipse/sumo/issues/1331.\n",
    "'freq' attribute of the calibrator may need to be increased as there is a trade of between small and large time\n",
    "intervals in which to calibrate the flow.\n",
    "\"While this can be done with dfrouter as well, the method described here is more robust for highly meshed networks as found in cities\"\n",
    "\"For the calibrator to be able to function before the first vehicle, it needs a fall back route which just needs to consist of a single edge (i.e. the edge on which the calibrator is placed).\"\n",
    "\"However, the realism of traffic flow behind (or between) calibrators depends on the fit between random routes and real-world routes.\"\n",
    "'''\n",
    "\n",
    "generate_calibrator_file(sumo_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dd2d7-5de8-446e-9bd8-b4b6636f93dd",
   "metadata": {},
   "source": [
    "# 5. Extracting Simulation Data\n",
    "In this section, data from the simulation will be extracted and visualised. First, the simulation must be run; then TraCI and sumolib can be used to interact live and summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d9503-eb5f-424b-95d4-9670ee5a2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Change config file to include <seed value=\"202\"/> such as to fix the stochasticity for reproducable results'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ee079-4797-484a-8663-51f75cec434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd C:\\Users\\ollir\\OneDrive\\Documents\\University\\Data Science MSc\\Dissertation\\To-Fly-or-Not-to-Fly\\simulation\n",
    "!sumo -c osm.sumocfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d7660d2-7f45-48c7-85d5-819f44c87c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_2_csv(files):\n",
    "    '''\n",
    "    Function to convert a list of XML files to CSV.\n",
    "    '''\n",
    "    for file, out_name in files:\n",
    "        !xml2csv.py {file} --output {out_name}\n",
    "    \n",
    "files = [('tripinfo_output.xml','tripinfo_output.csv'),\n",
    "         ('statistic_output.xml','statistic_output.csv'),\n",
    "         ('edge_output.xml','edge_output.csv')]\n",
    "xml_2_csv(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ac37980-2986-47b9-bbe7-07fdc7a77cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_output = pd.read_csv('tripinfo_output.csv', sep=';', header=0, index_col=None) #, dtype=col_types, usecols=cols, na_values='')\n",
    "stats_output = pd.read_csv('statistic_output.csv', sep=';', header=0, index_col=None) #, dtype=col_types, usecols=cols, na_values='')\n",
    "edge_output = pd.read_csv('edge_output.csv', sep=';', header=0, index_col=None) #, dtype=col_types, usecols=cols, na_values='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe35573-59f6-4e69-87ae-57221fe64a8f",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "- ~~Change above function to return edge id~~\n",
    "- ~~Also need to work out position along edge for detector file (going for length/2)~~\n",
    "- ~~How do you choose the value for a certain count point?~~\n",
    "- ~~Compute detector information and generate detector file. **Note: add type**~~\n",
    "- ~~Compute flow information and generate flow file~~\n",
    "- ~~Use flowrouter.py~~\n",
    "\n",
    "**Try using [dynamic calibrators](https://sumo.dlr.de/docs/Simulation/Calibrator.html#Calibrators) as per [this recommendation](https://www.researchgate.net/post/How-cab-I-build-a-realistic-road-traffic-scenario-using-real-traffic-data-and-SUMO).**\n",
    "- ~~Generate traffic using randomTrips.py~~\n",
    "- ~~Could validate these routes if necessary.~~\n",
    "- ~~Create calibrators, according to traffic counter positions in the network.~~\n",
    "- ~~Use routeprobes too~~\n",
    "\n",
    "See [this page of the wiki](https://sumo.dlr.de/docs/Simulation/Calibrator.html#building_a_scenario_without_knowledge_of_routes_based_on_flow_measurements).\n",
    "Running the simulation with the random demand as well as these `<calibrator>` and `<routeProbe>` definitions will achieve a simulation in which traffic matches the specified flows at each calibrator edge. However, the realism of traffic flow behind (or between) calibrators depends on the fit between random routes and real-world routes. **The importance of this fit increases with the size and complexity of the network between calibrator edges.**\n",
    "\n",
    "- Trial sublane lane changing model for EV\n",
    "- Plan visualisations from information extracted from simulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
