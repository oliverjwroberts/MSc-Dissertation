{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037deac0-c3f5-4749-8829-8021d0b889f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b7033-13a2-4de1-be00-b7417fb4a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import time\n",
    "import datetime\n",
    "import sklearn\n",
    "import statsmodels\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neural_network\n",
    "import sklearn.ensemble\n",
    "import sklearn.gaussian_process\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "# Check SUMO_HOME is set\n",
    "import os, sys\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "    \n",
    "import traci\n",
    "import sumolib\n",
    "import altair as alt\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab34f2d-7e72-4fb7-a555-57f7c0444e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'count_point_id': 'string',\n",
    "    'direction_of_travel': 'string',\n",
    "    'count_date': 'string',\n",
    "    'hour': 'string',\n",
    "    'road_name': 'string',\n",
    "    'road_type': 'string',\n",
    "    'latitude': 'float',\n",
    "    'longitude': 'float',\n",
    "    'link_length_km': 'float',\n",
    "    'pedal_cycles': 'int',\n",
    "    'two_wheeled_motor_vehicles': 'int',\n",
    "    'cars_and_taxis': 'int',\n",
    "    'buses_and_coaches': 'int',\n",
    "    'lgvs': 'int',\n",
    "    'all_hgvs': 'int',\n",
    "    'all_motor_vehicles': 'int' \n",
    "}\n",
    "\n",
    "cols = list(col_types.keys())\n",
    "\n",
    "dft_counts = pd.read_csv('dft_count_swansea.csv', sep=',', header=0,\n",
    "                         index_col=None, dtype=col_types, usecols=cols, na_values='')\n",
    "dft_counts['count_date'] = pd.to_datetime(dft_counts['count_date'], format= '%Y-%m-%d')\n",
    "dft_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a58e0-c669-4055-a238-e330483db9de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Data Analysis and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba7317-06ca-40b0-a56d-094c949b4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack(df):\n",
    "    # unstack hourly counts\n",
    "    unstacked_df = df.set_index(['count_point_id','direction_of_travel','count_date','hour'])\n",
    "    unstacked_df = unstacked_df[['all_motor_vehicles']]\n",
    "    unstacked_df = unstacked_df.unstack(level=-1)\n",
    "    unstacked_df.reset_index(inplace=True)\n",
    "    \n",
    "    # flatten multi-level col index and rename\n",
    "    unstacked_df.columns = unstacked_df.columns.to_flat_index()\n",
    "    col_names = [a for a in unstacked_df.columns]\n",
    "    col_names = [name[0] if col_names.index(name) <= 2 else name[1] for name in col_names]\n",
    "    unstacked_df.columns = col_names\n",
    "    ordered_col_names = ['count_point_id','direction_of_travel','count_date',\n",
    "                         '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "    unstacked_df = unstacked_df[ordered_col_names]\n",
    "    return unstacked_df\n",
    "\n",
    "unstacked_counts = unstack(dft_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e21c22-1a5b-4527-adff-57b9be6c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cp_date_ranges(df):\n",
    "    cps = [cp_id for cp_id in pd.unique(df['count_point_id'])]\n",
    "    min_date = []\n",
    "    max_date = []\n",
    "    for cp_id in cps:\n",
    "        min_date.append(df.loc[df['count_point_id'] == cp_id]['count_date'].min())\n",
    "        max_date.append(df.loc[df['count_point_id'] == cp_id]['count_date'].max())\n",
    "    cp_date_ranges = pd.DataFrame(\n",
    "        {\n",
    "            'cp_id': cps,\n",
    "            'min_date': min_date,\n",
    "            'max_date': max_date\n",
    "        })\n",
    "    return cp_date_ranges\n",
    "\n",
    "\n",
    "def get_cp_dates(df):\n",
    "    cps = [cp_id for cp_id in pd.unique(df['count_point_id'])]\n",
    "    cps_longform = []\n",
    "    dates = []\n",
    "    for cp_id in cps:\n",
    "        filtered_df = df.loc[df['count_point_id'] == cp_id]\n",
    "        cps_longform = cps_longform + [i for i in df.loc[df['count_point_id'] == cp_id]['count_point_id']]\n",
    "        dates = dates + [i for i in df.loc[df['count_point_id'] == cp_id]['count_date']]\n",
    "    cp_dates = pd.DataFrame(\n",
    "        {\n",
    "            'cp_id': cps_longform,\n",
    "            'dates': dates\n",
    "        })\n",
    "    return cp_dates\n",
    "\n",
    "cp_date_ranges = get_cp_date_ranges(unstacked_counts)\n",
    "cp_dates = get_cp_dates(unstacked_counts)\n",
    "\n",
    "# Visualise\n",
    "selector = alt.selection_interval(encodings=['y'])\n",
    "\n",
    "dates_range = alt.Chart(cp_date_ranges).mark_bar().encode(\n",
    "    x=alt.X('cp_id:O', title='Count Point ID'),\n",
    "    y=alt.Y('min_date', title='Minimum - Maximum Date'),\n",
    "    y2=alt.Y2('max_date', title=None),\n",
    "    tooltip=[ 'cp_id','min_date', 'max_date']\n",
    ").properties(\n",
    "    title = 'Count Point Active Dates',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "dates = alt.Chart(cp_dates).mark_circle(color='orange').encode(\n",
    "    x=alt.X('cp_id:O'),\n",
    "    y=alt.Y('dates'),\n",
    "    tooltip=['cp_id','dates']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ").add_selection(\n",
    "    selector\n",
    ")\n",
    "\n",
    "dates_range + dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb73b3b-fd32-409f-b6ab-9c4661fdeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group counts by cp id, year, and direction of travel and compute the mean for each group\n",
    "# this actually doesnt reduce the number of rows at all\n",
    "grouped_counts = unstacked_counts.groupby(['count_point_id',\n",
    "                                           unstacked_counts['count_date'].dt.year,\n",
    "                                           'direction_of_travel']).mean().reset_index()\n",
    "\n",
    "# copy grouped counts and drop non count data\n",
    "counts_for_model = grouped_counts.drop(columns=['count_point_id','count_date','direction_of_travel'])\n",
    "counts_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e648633-c441-496f-b1b5-a99ff6515151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers(data):\n",
    "    '''\n",
    "    Function to count the number of outliers (according to the scipy z-score)\n",
    "    found within the traffic count data.\n",
    "    \n",
    "    '''\n",
    "    outlier_count = {}\n",
    "    for col in data.columns:\n",
    "        bools = (np.abs(stats.zscore(data[col].astype(float))) < 3)\n",
    "        if col not in outlier_count:\n",
    "            outlier_count[col] = len(data)-len(data[bools])\n",
    "        \n",
    "    return outlier_count\n",
    "   \n",
    "count_outliers(counts_for_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f737a1-c63f-4272-81f3-38b7d73d78a6",
   "metadata": {},
   "source": [
    "Although it appears there are several outliers present across all CPs for each hour, as shown below, these are not truly outliers. The abnormally large values are just from high traffic flow sections of the network, like the M4. If we were to remove the outliers, this would remove 4 specific count points, thus loosing detail about those specific locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f00747-f703-47f6-b111-0018e3829aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = (np.abs(stats.zscore(counts_for_model.astype(float))) < 3).all(axis=1)\n",
    "idx = np.where(bools==False)\n",
    "print(pd.unique(grouped_counts.iloc[idx]['count_point_id']))\n",
    "grouped_counts.iloc[idx].loc[grouped_counts['count_point_id']=='40504'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9498cb9-0d26-4156-9765-1101fe88fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix numpy seed\n",
    "SEED = 202\n",
    "np.random.seed(SEED) \n",
    "\n",
    "# split data into train and test sets\n",
    "x = np.array(counts_for_model.drop('18', axis=1))\n",
    "y = np.array(counts_for_model['18'])\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y, test_size=0.4, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bfae3-2e15-46d5-a7d2-6dc9b7bcac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(scaler_type='robust'):\n",
    "    '''\n",
    "    Create and return sklearn scaler object that is fitted to the x_trian data. \n",
    "    Various scalers can be used, just pass a different string to scaler_type arg. \n",
    "    \n",
    "    '''\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    elif scaler_type == 'robust':\n",
    "        scaler = sklearn.preprocessing.RobustScaler()\n",
    "        \n",
    "    # fit scaler to train data\n",
    "    scaler.fit(x_train)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def scale_data(scaler, data_to_be_scaled):\n",
    "    '''Function to scale data using the chosen scaler.'''\n",
    "    scaled_data = scaler.transform(data_to_be_scaled)\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "# create scaler\n",
    "scaler = get_scaler(scaler_type='robust')\n",
    "\n",
    "# scale data\n",
    "scaled_x_train = scale_data(scaler, x_train)\n",
    "scaled_x_test = scale_data(scaler, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e9ad3-ff5f-40a0-bc28-cdf106d09410",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Traffic Prediction\n",
    "In this section various models are trained to predict the 11th hour traffic counts given the previous 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d818ba-7710-42be-af76-8668474fbed9",
   "metadata": {},
   "source": [
    "## 3.1 Linear Regression\n",
    "Baseline linear regression model.\n",
    "\n",
    "We try creating an autoregression model. See [this machine learning mastery tutorial](https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/) for more details. \n",
    "\n",
    "> Because the regression model uses data from the same input variable at previous time steps, it is referred to as an autoregression (regression of self).\n",
    "\n",
    "Alternativly, the [HCrystal Ball package](https://hcrystalball.readthedocs.io/en/stable/examples/tutorial/wrappers/02_ar_modelling_in_sklearn.html) could be used to wrap any sklearn regression model as an autoregressive model.\n",
    "\n",
    "Some good papers that talk about previous models being used:\n",
    "- [Gaussian Process Regression Based Traffic Modeling and Prediction in High-Speed Networks](https://ieeexplore.ieee.org/abstract/document/7841857)\n",
    "- [Spatiotemporal Traffic Prediction Using Hierarchical Bayesian Modeling](https://www.mdpi.com/1999-5903/13/9/225/htm)\n",
    "- [This one](http://thomas-liebig.eu/wordpress/wp-content/papercite-data/pdf/schnitzler14.pdf)\n",
    "- [A bayesian network approach to traffic flow forecasting](https://ieeexplore.ieee.org/abstract/document/1603558?casa_token=EaS_xbcxIzEAAAAA:S4JezzP0kpRUWmj8ura7R1Pn89vXLN6Z-LPeJ1MmDOcbGymxb-q_WQCqg8jFZUc4IOD4_eb4g8g)\n",
    "- Ridge regression model directly from sklearn.\n",
    "\n",
    "### Grid Search CV\n",
    "For attributes of the grid search results, look at the [Grid Search CV page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=grid%20search%20cv#sklearn.model_selection.GridSearchCV) on the sklearn website. Similarly, for an explanation of CV, look at the [releveant sklearn user guide page](https://scikit-learn.org/stable/modules/cross_validation.html#). For an explanation of the metrics found in the `gscv` object, please refer to the 'returns' section of [this page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate).\n",
    "\n",
    "[This answer](https://stats.stackexchange.com/a/378464) cleared it up beautifully! \n",
    "\n",
    "[This documentation on storemagic](https://ipython.org/ipython-doc/rel-0.12/config/extensions/storemagic.html) can be used to store and load grid search CV variables to save on computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9e039-788b-423f-94e1-c1599ba8f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(model, params, x_train=scaled_x_train, y_train=y_train):\n",
    "    '''\n",
    "    Function to use Grid Search Cross Validation (GSCV) to optimise the hyperparameters\n",
    "    of the sklearn models. The GSCV object is returned.\n",
    "    \n",
    "    '''\n",
    "    start = time.time()\n",
    "    grid_search = sklearn.model_selection.GridSearchCV(model, params, n_jobs=5, cv=5,\n",
    "                                                      return_train_score=True, verbose=4)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(f'Found optimal hyperparameter combination for {model} in {np.round(time.time()-start,4)} seconds')\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1447f4-33a8-4c1c-b927-ec104de81295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# params\n",
    "lr_params = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False],\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "# optimse\n",
    "lr_gscv = optimise(lr, lr_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d7fd4-9932-4027-8237-8a1bc6ec0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sklearn_ypred(model_gscv):\n",
    "    '''\n",
    "    Function to get predicted values from the sklearn models.\n",
    "    This is done by extracting the best estimator from the gridsearchCV\n",
    "    objects, and predicting the y values from the x_test set.\n",
    "    \n",
    "    '''\n",
    "    best_estimator_idx = model_gscv.best_index_\n",
    "    y_pred = model_gscv.best_estimator_.predict(scaled_x_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64460726-125b-4b75-aa17-c909de6ae810",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741fc24-528a-4c4c-8b91-ad01ec5acd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get y predictions\n",
    "lr_y_pred = get_sklearn_ypred(lr_gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab66990-afbb-448a-8793-e571215aed9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2 Random Forest\n",
    "The next model to be used is a Random Forest. More information about it can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808c2a0-f03a-4bca-8e59-f417998603b7",
   "metadata": {},
   "source": [
    "```python\n",
    "# model to be optimised\n",
    "rf = sklearn.ensemble.RandomForestRegressor()\n",
    "\n",
    "# parameters to search in dicts\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'criterion': ['mse','mae'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_gscv = optimise(rf, rf_params) # gscv stands for grid search cross validation\n",
    "# store gscv object to access across kernel sessions\n",
    "%store rf_gscv\n",
    "\n",
    "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
    "Found optimal hyperparameter combination for RandomForestRegressor() in 220.164 seconds\n",
    "Stored 'rf_gscv' (GridSearchCV)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196dd3d-7967-4072-ae1c-9f17ad19f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gscv object from store as it takes a while otherwise. \n",
    "%store -r rf_gscv\n",
    "rf_gscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d1219-d4b6-40df-9739-7b1e85b419c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317530e1-bc46-4770-a18a-abb6aa53f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred = get_sklearn_ypred(rf_gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08576e70-cbc5-4759-aca4-0cb7f9eeb3e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.2 (Alt) Neural Network\n",
    "In this section we shall try a neural network model from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0af5a2-d51b-4172-a814-3295e2008b6f",
   "metadata": {},
   "source": [
    "```python\n",
    "# mlp model\n",
    "mlp = sklearn.neural_network.MLPRegressor()\n",
    "\n",
    "# params\n",
    "mlp_params = {\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.00001, 0.0001, 0.001],\n",
    "    'learning_rate': ['constant',' invscaling', 'adaptive'],\n",
    "    'max_iter': [100, 250, 500, 1000]\n",
    "}\n",
    "    \n",
    "mlp_gscv = optimise(mlp, mlp_params)\n",
    "%store mlp_gscv\n",
    "\n",
    "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
    "Found optimal hyperparameter combination for MLPRegressor() in 298.5623 seconds\n",
    "Stored 'mlp_gscv' (GridSearchCV)\n",
    "\n",
    "\n",
    "# get gscv object from store\n",
    "%store -r mlp_gscv\n",
    "mlp_gscv\n",
    "\n",
    "mlp_y_pred = get_sklearn_ypred(mlp_gscv)\n",
    "mlp_gscv.best_score_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2a4dd-42ec-42df-9405-f0d6cbf0674e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.2 (Alt) Gaussian Process\n",
    "Here we use a Gaussian Process Regression (GPR) model. More information about it can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6b5b3-c469-4df7-99a2-1b80988fcb1b",
   "metadata": {},
   "source": [
    "```python\n",
    "# model\n",
    "gp = sklearn.gaussian_process.GaussianProcessRegressor()\n",
    "\n",
    "# params\n",
    "gp_params = {\n",
    "    #'kernel': [sklearn.gaussian_process.kernels.DotProduct(), \n",
    "    #           sklearn.gaussian_process.kernels.ConstantKernel()],\n",
    "    'normalize_y': [False, True]\n",
    "}\n",
    "\n",
    "gp_gscv = optimise(gp, gp_params)\n",
    "\n",
    "gp_y_pred = get_sklearn_ypred(gp_gscv)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac87f8-747d-41f0-a3a1-1f922de82a69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3 CNN-LSTM\n",
    "In this section we use a Convolutional Nerual Network with Long-Short-Term-Memory layers to predict the traffic flow. This is implemented using Tensorflow. \n",
    "\n",
    "To use sklearn's Grid Search Cross Validation, the model needs to wrapped. More information can be found [here](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/).\n",
    "- [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html#) \n",
    "\n",
    "First of all, the data needs to be scaled - feature wise. It is not advised to scale the row of data as then you lose the realtionships between the features for each row of data as they will be different. There are various scalers out there, [this page](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py) compares all the scalers sklearn offer - may be a good starting point for the discussion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd3761-08e1-4a28-add5-f3a56c7c1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(x_data, verbose=False):\n",
    "    '''\n",
    "    Function to reshape data into a format compatable with Conv1DLSTM layers.\n",
    "    Expected shape = (samples, time, rows, channels)\n",
    "    This reshaping is necessary for all input (x) data. \n",
    "    \n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f'Original data shape: {x_data.shape}')\n",
    "        print(f'One sample shape: {x_data.shape[0]}')\n",
    "        print(f'An example sample: {x_data[0]}')\n",
    "    \n",
    "    sample_size = x_data.shape[0] # number of samples in x data\n",
    "    time_steps  = x_data.shape[1] # number of features in x data\n",
    "    input_dimension = 1 # each feature is represented by 1 number\n",
    "    \n",
    "    reshaped_x_data = x_data.reshape(sample_size,time_steps,input_dimension)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Reshaped data shape: {reshaped_x_data.shape}')\n",
    "        print(f'One sample shape: {reshaped_x_data.shape[0]}')\n",
    "        print(f'An example sample: {reshaped_x_data[0]}')    \n",
    "    \n",
    "    return reshaped_x_data\n",
    "\n",
    "# reshape scaled x data\n",
    "reshaped_x_train = reshape_data(scaled_x_train, verbose=True)\n",
    "reshaped_x_test = reshape_data(scaled_x_test, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb34e7-2f6a-4528-bcb4-5255e2f9bbd1",
   "metadata": {},
   "source": [
    "After conversion, the train data has a shape:\n",
    "`[n_samples, n_features (time steps), input_dimension] ---> [811, 11, 1]`\n",
    "That is, each sample has **11 time steps, each with 1 input dimension**.\n",
    "\n",
    "Although it applies to a different problem, please see the [this tutorial notebook](https://colab.research.google.com/drive/1zjh0tUPYJYgJJunpLC9fW5uf--O0LKeZ?usp=sharing#scrollTo=ydUsxRzwNl4-) and the example GIF:\n",
    "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/conv1d.gif?raw=true\" width=\"500\">\n",
    "\n",
    "Other potentially good papers:\n",
    "- [Short-Term Traffic Prediction with Vicinity Gaussian Process in the Presence of Missing Data](https://ieeexplore.ieee.org/abstract/document/8547118)\n",
    "- [A CNN-LSTM-Based Model to Forecast Stock Prices](https://www.hindawi.com/journals/complexity/2020/6622927/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acce33-4895-48f0-8527-eb0678a04235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # compute number of timesteps and features to pass to model\n",
    "    n_features = reshaped_x_train.shape[1] # number of features -> 11 in this case\n",
    "    n_dimension = reshaped_x_train.shape[2] # each feature is represented by 1 number\n",
    "\n",
    "    model = tf.keras.Sequential(layers=[\n",
    "        tf.keras.layers.InputLayer(input_shape=(n_features, n_dimension)),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "        tf.keras.layers.LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "        tf.keras.layers.LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "        tf.keras.layers.LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "               \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=n_dimension, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "cnn_model = create_model()\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6b090-272f-478b-8c23-0bf209f5497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimisier = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = 'mse'\n",
    "metric = ['mae']\n",
    "\n",
    "# compile model\n",
    "cnn_model.compile(optimizer=optimisier,\n",
    "                  loss=loss, \n",
    "                  metrics=metric)\n",
    "\n",
    "# fit to training data and record output\n",
    "history = cnn_model.fit(reshaped_x_train, y_train, epochs=100, verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32deee-7f77-4bcc-997d-1ab199d86aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metric):\n",
    "    # convert history dict to df\n",
    "    data = pd.DataFrame(history.history)\n",
    "    data['epoch'] = history.epoch\n",
    "    \n",
    "    # visualise metric\n",
    "    train_val_metric = alt.Chart(data).transform_fold(\n",
    "        [metric, 'val_'+metric],\n",
    "        as_=['Type', 'Value']\n",
    "    ).mark_line(clip=True).encode(\n",
    "        alt.X('epoch:O', title='Epoch'),\n",
    "        alt.Y('Value:Q',title='Mean Average Error', scale=alt.Scale(domain=(0, 200))),\n",
    "        alt.Color('Type:N', legend=None)\n",
    "    ).properties(\n",
    "    width=300,\n",
    "    height=300\n",
    "    )\n",
    "    \n",
    "    # visualise loss\n",
    "    train_val_loss = alt.Chart(data).transform_fold(\n",
    "        ['loss', 'val_loss'],\n",
    "        as_=['Type', 'Value']\n",
    "    ).mark_line(clip=True).encode(\n",
    "        alt.X('epoch:O', title='Epoch'),\n",
    "        alt.Y('Value:Q', title='Loss', scale=alt.Scale(domain=(0, 100000))),\n",
    "        alt.Color('Type:N')\n",
    "    ).properties(\n",
    "    width=300,\n",
    "    height=300\n",
    "    )\n",
    "    \n",
    "    return alt.hconcat(train_val_metric, train_val_loss).resolve_scale(color='independent')\n",
    "\n",
    "    \n",
    "plot_history(history, 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cb632-3a47-4c8b-9048-47aaf481dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_y_pred = cnn_model.predict(reshaped_x_test).flatten()\n",
    "results = cnn_model.evaluate(reshaped_x_test, y_test, verbose=0)\n",
    "print(f'Test set {loss}: {results[0]:0.2f}')\n",
    "print(f'Test set {metric}: {results[1]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe74aeb-9cd7-423c-908e-94ba48f1de5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.4 Model Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618fb26-3a97-48c4-bb97-c5e3f80fd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(predictions, model_names):\n",
    "    \n",
    "    # check predictions and mode_names are same length\n",
    "    if len(predictions) != len(model_names):\n",
    "        raise ValueError('List of predictions and model names need to be same length')\n",
    "\n",
    "    # create dataframe of results for each model\n",
    "    list_of_dfs = []\n",
    "    for i in range(len(predictions)):\n",
    "        error = y_test - predictions[i]\n",
    "        model_name = model_names[i]\n",
    "        model_data = pd.DataFrame({'x': y_test, 'y': predictions[i],\n",
    "                                   'error': error, 'model_name': model_name})\n",
    "        list_of_dfs.append(model_data)\n",
    "\n",
    "    # concat all individual dfs into one to pass to vis\n",
    "    data = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualise_predictions(predictions, model_names):\n",
    "    '''\n",
    "    Function to visualise a true vs predicted scatter and histogram of errors\n",
    "    for each model and prediction in the passed lists.\n",
    "    \n",
    "    '''\n",
    "    data = prep_data(predictions, model_names)\n",
    "\n",
    "    vis = alt.concat(*(\n",
    "        # true vs predicted\n",
    "        alt.Chart(data.loc[data['model_name']==name], title=name+' True vs Predicted'\n",
    "        ).mark_circle().encode(\n",
    "            alt.X('x', title='Actual Counts'),\n",
    "            alt.Y('y', title='Predected Counts'),\n",
    "            color='model_name:N',\n",
    "            tooltip=['model_name', 'x', 'y']\n",
    "        ).properties(\n",
    "            width=300,\n",
    "            height=300\n",
    "        )\n",
    "        +\n",
    "        # x=y line\n",
    "        alt.Chart(pd.DataFrame({'x': [0,3500],\n",
    "                                'y': [0,3500]})\n",
    "                        ).mark_line(\n",
    "            color='grey',\n",
    "            clip=True\n",
    "        ).encode(\n",
    "            alt.X('x'),#, scale=alt.Scale(domain=(0, max_value))),\n",
    "            alt.Y('y'),#, scale=alt.Scale(domain=(0, max_value)))\n",
    "        )\n",
    "        |\n",
    "        # histogram\n",
    "        alt.Chart(data.loc[data['model_name']==name], title=name+' Error Histogram'\n",
    "        ).mark_bar().encode(\n",
    "            alt.X('error:Q', bin=alt.Bin(maxbins=100, extent=[-500, 500]), title='Prediction Error'),\n",
    "            alt.Y('count()', title='Count'),\n",
    "            color='model_name:N',\n",
    "            tooltip=['model_name','error','count()']\n",
    "        ).properties(\n",
    "            width=300,\n",
    "            height=300\n",
    "        )\n",
    "        for name in model_names # [::-1] if want to reverse order\n",
    "      ), columns=1\n",
    "    ).properties(\n",
    "        title = 'Model Test Results',\n",
    "    )    \n",
    "    return vis\n",
    "\n",
    "\n",
    "predictions = [lr_y_pred, rf_y_pred, cnn_y_pred]\n",
    "model_names = ['LR', 'RF', 'CNN']\n",
    "visualise_predictions(predictions, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be569a2d-723d-464c-bcd1-0e5b24efcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y_test, y_pred):\n",
    "    '''\n",
    "    Function to replace inf values caused by the division of zero\n",
    "    with zero so the actual mean can be returned.\n",
    "    \n",
    "    '''\n",
    "    calculation = 100*abs((y_test-y_pred)/y_test)\n",
    "    for idx, value in enumerate(calculation):\n",
    "        if np.isinf(value):\n",
    "            calculation.iloc[idx] = 0\n",
    "            \n",
    "    mape = np.mean(calculation) \n",
    "    return str(np.round(mape,2))+'%'\n",
    "\n",
    "\n",
    "def get_test_measures(predictions, model_names):\n",
    "    '''\n",
    "    Function to calculate standard statistical measures for each model\n",
    "    and report them as a DataFrame.\n",
    "    \n",
    "    '''\n",
    "    data = prep_data(predictions, model_names)\n",
    "    list_of_measures = []\n",
    "    for name in model_names:\n",
    "        filtered_data = data.loc[data['model_name']==name]\n",
    "        y_pred = filtered_data['y']\n",
    "        measures_dict = {\n",
    "            'Model Name': name,\n",
    "            #'Train Time': trainTime,\n",
    "            'MSE': np.round(np.mean((y_test-y_pred)**2),2),\n",
    "            'RMSE': np.round(np.sqrt(np.mean((y_test-y_pred)**2)),2),\n",
    "            'MAE': np.round(np.mean(abs(y_test-y_pred)),2),\n",
    "            # mean absolute percentage error\n",
    "            'MAPE': get_mape(y_test, y_pred),\n",
    "            #'R2': model.score(x, y)\n",
    "        }\n",
    "        list_of_measures.append(pd.DataFrame(measures_dict, index=[0]))\n",
    "        \n",
    "    measures_df = pd.concat(list_of_measures, ignore_index=True)\n",
    "    measures_df.set_index('Model Name', inplace=True)\n",
    "    return measures_df\n",
    "\n",
    "\n",
    "measures_df = get_test_measures(predictions, model_names)\n",
    "measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c20ea-a2d2-4c7c-895b-cb0b0e2bde60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Simulation\n",
    "In this section we build and run the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e4dac-9b66-487f-a0fa-41105c1d09ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Network\n",
    "We build the network using OSM data. There are many ways to import a network through OSM data which are discussed on [this page OpenStreetMap](https://sumo.dlr.de/docs/Networks/Import/OpenStreetMap.html). This method required finding the area reference for Swansea (3600087944) and using the [Overpass API Query Form](http://www.overpass-api.de/query_form.html) to download the OSM data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69981937-cfe6-427f-85e3-77714761f980",
   "metadata": {},
   "source": [
    "```\n",
    "!netconvert --osm-files swansea.osm.xml -o swansea.net.xml --lefthand --geometry.remove --ramps.guess --junctions.join --tls.guess-signals --tls.discard-simple --tls.join --tls.default-type actuated --keep-edges.by-vclass passenger -v\n",
    "!polyconvert --net-file swansea.net.xml --osm-files swansea.osm.xml --type-file osmPolyconvert.typ.xml -o swansea.poly.xml\n",
    "\n",
    "\n",
    "Warning: Discarding unknown compound 'usage.main' in type 'railway.rail|usage.main' (first occurence for edge '3993283').\n",
    "Warning: Discarding unknown compound 'usage.branch' in type 'railway.rail|usage.branch' (first occurence for edge '3993295').\n",
    "Warning: Discarding unusable type 'waterway.stream' (first occurence for edge '4673837').\n",
    "Warning: Discarding unknown compound 'cycleway.track' in type 'cycleway.track|highway.secondary' (first occurence for edge '5528275').\n",
    "Warning: Discarding unknown compound 'cycleway.lane' in type 'cycleway.lane|highway.service' (first occurence for edge '11359098').\n",
    "Warning: Discarding unusable type 'highway.construction' (first occurence for edge '12628117').\n",
    "Warning: Discarding unknown compound 'cycleway.lane' in type 'cycleway.lane|highway.residential' (first occurence for edge '13851148').\n",
    "Warning: Discarding unusable type 'waterway.canal' (first occurence for edge '23887449').\n",
    "Warning: Discarding unusable type 'waterway.river' (first occurence for edge '23887457').\n",
    "    \n",
    "-----------------------------------------------------\n",
    "Summary:\n",
    " Node type statistics:\n",
    "  Unregulated junctions       : 0\n",
    "  Dead-end junctions          : 14\n",
    "  Priority junctions          : 8033\n",
    "  Right-before-left junctions : 707\n",
    "  Traffic light junctions      : 86\n",
    " Network boundaries:\n",
    "  Original boundary  : -4.33,51.54,-3.84,51.78\n",
    "  Applied offset     : -410710.40,5710965.84\n",
    "  Converted boundary : 0.00,-25813.54,31168.98,0.00\n",
    "-----------------------------------------------------\n",
    "Writing network ... done (2784ms).\n",
    "Success\n",
    "\n",
    "Warning: 55 total messages of type: Ambiguity in turnarounds computation at junction '%'.\n",
    "Warning: 29 total messages of type: Discarding unknown compound '%' in type '%' (first occurence for edge '%').\n",
    "Warning: 22 total messages of type: Discarding unusable type '%' (first occurence for edge '%').\n",
    "Warning: 42 total messages of type: Found angle of % degrees at edge '%', segment %.\n",
    "Warning: 70 total messages of type: Found sharp turn with radius % at the end of edge '%'.\n",
    "Warning: 81 total messages of type: Found sharp turn with radius % at the start of edge '%'.\n",
    "Warning: 38 total messages of type: Ignoring restriction relation '%'.\n",
    "Warning: 105 total messages of type: Intersecting left turns at junction '%' from lane '%' and lane '%' (increase junction radius to avoid this).\n",
    "Warning: 31 total messages of type: Minor green from edge '%' to edge '%' exceeds %m/s. Maybe a left-turn lane is missing.\n",
    "Warning: 65 total messages of type: Not joining junctions % (%).\n",
    "Warning: 21 total messages of type: Reducing junction cluster % (%).\n",
    "Warning: 249 total messages of type: Speed of % connection '%' reduced by % due to turning radius of % (length=%, angle=%).\n",
    "Warning: 24 total messages of type: from-edge '%' of restriction relation could not be determined\n",
    "Warning: 10 total messages of type: to-edge '%' of restriction relation could not be determined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278616d0-d778-4c20-928b-44e831da898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = sumolib.net.readNet('swansea.net.xml') # read net file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7c4a8-69b1-462d-98d9-52d0adb47b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2 Traffic Demand\n",
    "It was most appropriate to generate SUMO traffic demand by using randomTrips.py and Calibrators. First, a file containing random routes are created from the python script. Then a calibrator is created for each count point and adds or removes vehicles to the simulation depending on the assigned traffic count that it needs to achieve.\n",
    "\n",
    "`randomTrips.py` can be called using the following cmd command:\n",
    "\n",
    "**Note: When using --validate, trip files are generated which results in the simulation running very slowly.** Just use route files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177e836-c555-4082-b9f5-5ae8640c3afd",
   "metadata": {},
   "source": [
    "### 4.2.1 Predict Traffic and Data Prep\n",
    "In this subsection, we shall predict the traffic using each model. The data shall then be prepared to pass to the randomTrips generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aec22f-b11f-4b5a-ba09-2fe4bbfcbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values(x_data, model, is_cnn=False):\n",
    "    '''\n",
    "    Function to predict values from the supplied x_data and model.\n",
    "    It expects the x_data to be unscaled.\n",
    "    If the model is the CNN-LSTM model, then reshaping, and a slightly\n",
    "    different prediction function is called.\n",
    "    \n",
    "    '''\n",
    "    x_data = scale_data(scaler, x_data)\n",
    "    if is_cnn:\n",
    "        x_data = reshape_data(x_data)\n",
    "        predictions = model.predict(x_data).flatten()\n",
    "        \n",
    "    else:\n",
    "        predictions = model.predict(x_data)  \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def prediction_df(model_list, x_data):\n",
    "    '''\n",
    "    Function to predict values from the supplied x_data \n",
    "    for each model in a list and return results in DataFrame.\n",
    "    \n",
    "    '''\n",
    "    prediction_dict = {}\n",
    "    for idx, model in enumerate(model_list):\n",
    "        name = model_names[idx]\n",
    "        if name=='CNN':\n",
    "            is_cnn=True\n",
    "        else:\n",
    "            is_cnn=False\n",
    "        prediction_dict[name] = predict_values(x_data, model, is_cnn=is_cnn)\n",
    "        \n",
    "    return pd.DataFrame(prediction_dict)\n",
    "\n",
    "\n",
    "model_list = [lr_gscv.best_estimator_, rf_gscv.best_estimator_, cnn_model] # same order as model names\n",
    "prediction_df = prediction_df(model_list, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa8c84-cb52-4dac-8512-fe2de5722f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predictions(counts_df, prediction_df):\n",
    "    '''\n",
    "    Function to merge the predictions to the counts df and return a new dw.\n",
    "    Only certain columns specified in counts_df_cols_to_keep are kept.\n",
    "    The data is then grouped by CP ID and directions of travel, and the mean is computed.\n",
    "    Finally, all count data is rounded to the nearest integer. \n",
    "    \n",
    "    '''\n",
    "    # merge data\n",
    "    counts_df_cols_to_keep = ['count_point_id','direction_of_travel','18']\n",
    "    merged_df = counts_df[counts_df_cols_to_keep].join(prediction_df)\n",
    "    \n",
    "    # group and compute the mean\n",
    "    merged_df = merged_df.groupby(['count_point_id', 'direction_of_travel']).mean().reset_index()\n",
    "    \n",
    "    # round columns\n",
    "    cols_to_round = ['18','LR','RF','CNN']\n",
    "    for col in cols_to_round:\n",
    "        merged_df[col] = np.round(merged_df[col]).astype(int)\n",
    "    merged_df.rename(columns={'18': 'Actual'}, inplace=True)\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "sumo_data = merge_predictions(grouped_counts, prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622e547-d186-46e6-ae28-f8e2667c526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cp_info(dft_counts):\n",
    "    '''\n",
    "    Function to extract cp location and road data from the original data.\n",
    "    \n",
    "    '''\n",
    "    cps = dft_counts.groupby('count_point_id').first().reset_index()\n",
    "    cols_to_keep = ['count_point_id', 'road_name', 'road_type','latitude', 'longitude']\n",
    "    return cps[cols_to_keep]\n",
    "\n",
    "\n",
    "def find_closest_edges(coordinates, net, radius=150, edges_to_return=2):\n",
    "    '''\n",
    "    Function to find closest n edges to specified coordinates.\n",
    "    \n",
    "    '''\n",
    "    # coordinates must be tuple of latitude then longitude\n",
    "    x, y = net.convertLonLat2XY(coordinates[1], coordinates[0]) # find x/y positions within network\n",
    "    edges = net.getNeighboringEdges(x, y, radius) # create list of edges in radius of point\n",
    "    \n",
    "    if len(edges) == 0:\n",
    "        raise ValueError(f'There were no edges found within the {radius} radius of {coordinates}')\n",
    "    \n",
    "    # find indices of n closest edges according to distance\n",
    "    distances = np.array([info[1] for info in edges])\n",
    "    min_dist_indices = np.argsort(distances)[:edges_to_return]\n",
    "    closest_edges = np.array(edges)[min_dist_indices] \n",
    "    closest_edges = [edge[0] for edge in closest_edges]\n",
    "    \n",
    "    return closest_edges\n",
    "\n",
    "\n",
    "cp_df = get_cp_info(dft_counts)\n",
    "cp_df['closest_edges'] = [find_closest_edges((lat, long), net, edges_to_return=2) \n",
    "                          for lat, long in zip(cp_df['latitude'], cp_df['longitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a892d-44e4-426c-b2dd-d892c93689aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_direction(edge):\n",
    "    '''\n",
    "    Function to determine which direction a specific edge is going. \n",
    "    \n",
    "    '''\n",
    "    from_x, from_y, from_z = edge._from._coord\n",
    "    to_x, to_y, to_z = edge._to._coord\n",
    "    dx = to_x - from_x\n",
    "    dy = to_y - from_y\n",
    "    if abs(dx) > abs(dy): # then edge direction is east or west\n",
    "        if to_x > from_x: \n",
    "            direction = 'E'\n",
    "        else: \n",
    "            direction = 'W'\n",
    "    if abs(dx) < abs(dy): # then edge direction is north or south\n",
    "        if to_y > from_y:\n",
    "            direction = 'N'\n",
    "        else:\n",
    "            direction = 'S'\n",
    "    return direction\n",
    "    \n",
    "\n",
    "def match_edges_directions(sumo_data, cp_df):\n",
    "    '''\n",
    "    Function to match edges with count point directions.\n",
    "    In some cases, count points have more than two directions when they are located \n",
    "    near junctions/roundabouts; here, the edge is set to NaN, and is dropped for simplicity.\n",
    "    Note: only two edges are retrieved in the find_closest_edges function. \n",
    "    \n",
    "    '''\n",
    "    start = time.time()\n",
    "    \n",
    "    edge_col_list = []\n",
    "    for cp_id in cp_df['count_point_id']:\n",
    "        cp_actual_dirs = [actual_dir for actual_dir in\n",
    "                          sumo_data.loc[sumo_data['count_point_id']==cp_id]['direction_of_travel']]\n",
    "        edges = cp_df.loc[cp_df['count_point_id']==cp_id]['closest_edges'].iloc[0]\n",
    "        edge_dirs = [get_edge_direction(edge) for edge in edges]\n",
    "        for direction in cp_actual_dirs:\n",
    "            if direction in edge_dirs:\n",
    "                idx = edge_dirs.index(direction)\n",
    "                cp_actual_dirs[cp_actual_dirs.index(direction)] = edges[idx]\n",
    "            else: \n",
    "                cp_actual_dirs[cp_actual_dirs.index(direction)] = np.nan\n",
    "        edge_col_list = edge_col_list + cp_actual_dirs\n",
    "    sumo_data['edge'] = edge_col_list\n",
    "    sumo_data.dropna(axis=0, inplace=True)\n",
    "    sumo_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f'Matched edges and directions in {np.round(time.time()-start, 2)} seconds')\n",
    "    return sumo_data\n",
    "\n",
    "\n",
    "sumo_data = match_edges_directions(sumo_data, cp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daa334-d0c6-413e-87f7-8c1346db3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_edge_speeds(sumo_data):\n",
    "    '''\n",
    "    Function to extract the average speed from each edge and impute as new column. \n",
    "    \n",
    "    '''\n",
    "    edge_speeds = [edge._speed for edge in sumo_data['edge']]\n",
    "    \n",
    "    MPS_TO_KPH = 3.6 # MPS to KPH    \n",
    "    # take 5 mph off max road speed to better represent avg speed of vehicles\n",
    "    # sumo_data['avg_speed'] = [speed - (5/MPH_TO_MPS) for speed in edge_speeds]\n",
    "    \n",
    "    sumo_data['avg_speed'] = edge_speeds\n",
    "    return sumo_data\n",
    "    \n",
    "sumo_data = impute_edge_speeds(sumo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46889ff-cbef-4636-a54c-f5131e2b5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cps(cp_df):\n",
    "    '''\n",
    "    Function to plot cps on a folium interactive map. \n",
    "    \n",
    "    '''\n",
    "    swansea_coords = [51.6195955, -3.9459248]\n",
    "    m = folium.Map(location=swansea_coords, zoom_start=11)   \n",
    "    for index, row in cp_df.iterrows():\n",
    "        folium.Marker((row.latitude, row.longitude), popup = row.count_point_id,\n",
    "                      icon=folium.Icon(color='black')).add_to(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "plot_cps(cp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d188b9e-a092-447a-9153-b032dc029cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090494be-4b9b-40fd-a3d1-70f6cdd277da",
   "metadata": {},
   "source": [
    "### 4.2.2 Generate Traffic Demand\n",
    "In this section we create the randomTrips and calibrator file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b780ad-e35e-4831-ad0a-26591cec5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWANSEA_NET_FILENAME = 'swansea.net.xml'\n",
    "TRAFFIC_TRIPS_FILENAME = 'traffic_trips.xml'\n",
    "TRAFFIC_ROUTES_FILENAME = 'traffic_routes.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba070e98-51d4-4581-881e-6c98bcc7651d",
   "metadata": {},
   "source": [
    "```\n",
    "!cd C:\\Users\\ollir\\OneDrive\\Documents\\University\\Data Science MSc\\Dissertation\\To-Fly-or-Not-to-Fly\\simulation\n",
    "!randomTrips.py -n {SWANSEA_NET_FILENAME} -o {TRAFFIC_TRIPS_FILENAME} --route-file {TRAFFIC_ROUTES_FILENAME} -e 3600 -p 0.25\n",
    "\n",
    "calling C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\duarouter -n swansea.net.xml -r traffic_trips.xml --ignore-errors --begin 0 --end 3600 --no-step-log --no-warnings -o traffic_routes.xml\n",
    "Success.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe0cd5-b38a-4813-914c-fa7a407ff97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATOR_FILENAME = \"calibrator.xml\"\n",
    "CALIBRATOR_OUTPUT_FILENAME = 'calibrator_output.xml'\n",
    "ROUTE_PROBE_OUTPUT_FILENAME = 'route_probe_output.xml'\n",
    "EDGE_OUTPUT_FILENAME = 'edge_output.xml'\n",
    "\n",
    "\n",
    "def generate_calibrator_file(model_name, sumo_data=sumo_data):\n",
    "    '''\n",
    "    Function to generate the SUMO detector file from the sumo_data df.\n",
    "    '''\n",
    "    FREQ = 1 # the aggregation interval in which to calibrate the flows. default is step-length\n",
    "    \n",
    "    with open(CALIBRATOR_FILENAME, 'w') as outf:\n",
    "        outf.write(\"<additional>\\n\")\n",
    "        \n",
    "        # route probes\n",
    "        for index, row in sumo_data.iterrows():\n",
    "            outf.write(f\"    <routeProbe id='{row.count_point_id+'_'+row.direction_of_travel+'_probe'}' edge='{row.edge._id}' freq='{FREQ}' file='{ROUTE_PROBE_OUTPUT_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        \n",
    "        # calibrators\n",
    "        for index, row in sumo_data.iterrows():\n",
    "            \n",
    "            outf.write(f\"    <calibrator id='{row.count_point_id+'_'+row.direction_of_travel}' edge='{row.edge._id}' pos='{row.edge._lanes[0]._length/2}' freq='{FREQ}' routeProbe='{row.count_point_id+'_'+row.direction_of_travel+'_probe'}' output='{CALIBRATOR_OUTPUT_FILENAME}'>\\n\".replace(\"'\",'\"'))\n",
    "            outf.write(f\"        <route id='{'fallback_'+row.count_point_id+'_'+row.direction_of_travel}' edges='{row.edge._id}'/>\\n\".replace(\"'\",'\"'))\n",
    "            outf.write(f\"        <flow  begin='0' end='3600' route='{'fallback_'+row.count_point_id+'_'+row.direction_of_travel}' vehsPerHour='{row[model_name]}' speed='{row.avg_speed}' vType='DEFAULT_VEHTYPE'/>\\n\".replace(\"'\",'\"'))\n",
    "            outf.write(\"    </calibrator>\\n\")\n",
    "        \n",
    "        # generate edge-based data output\n",
    "        outf.write(f\"    <edgeData id='measure_1' file='{EDGE_OUTPUT_FILENAME}'/>\\n\".replace(\"'\",'\"'))      \n",
    "        \n",
    "        outf.write(\"</additional>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59070eb2-77a7-4638-bf0a-99490e7e2164",
   "metadata": {},
   "source": [
    "note: [position of calibrator is actually ignored at the moment](https://github.com/eclipse/sumo/issues/1331).\n",
    "\n",
    "`freq` attribute of the calibrator may need to be increased as there is a trade of between small and large time intervals in which to calibrate the flow.\n",
    "\n",
    "\"While this can be done with dfrouter as well, the method described here is more robust for highly meshed networks as found in cities\"\n",
    "\n",
    "\"For the calibrator to be able to function before the first vehicle, it needs a fall back route which just needs to consist of a single edge (i.e. the edge on which the calibrator is placed).\"\n",
    "\n",
    "\"However, the realism of traffic flow behind (or between) calibrators depends on the fit between random routes and real-world routes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77bc0d-8320-4576-86d9-dea8a75ec4b6",
   "metadata": {},
   "source": [
    "### 4.2.3 Generate EV File\n",
    "The file describing the EV vehicle type and route to travel is generated in this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cd0ab-35c1-4f6d-88b6-8bf9d4931cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_FILENAME = 'emergency_vehicle.xml'\n",
    "\n",
    "\n",
    "def generate_ev_file():\n",
    "    '''\n",
    "    Function to generate the SUMO file describing the EV vehicle type\n",
    "    \n",
    "    '''\n",
    "    with open(EV_FILENAME, 'w') as outf:\n",
    "        outf.write(\"<routes>\\n\")\n",
    "        \n",
    "        # vType\n",
    "        outf.write(\"    <vType id='ev' length='5' maxSpeed='67' accel='4' sigma='0.2' speedFactor='1.5' guiShape='emergency' vClass='emergency'>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <param key='has.tripinfo.device' value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <param key='has.bluelight.device' value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <param key='has.fcd.device' value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </vType>\\n\")\n",
    "        \n",
    "        # this is from Morriston Hospital to Singleton Hospital\n",
    "        outf.write(\"    <trip id='ev_trip' type='ev' depart='600' from='-41432539#2' to='836332232#0'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    <!-- this is from Morriston Hospital -> Singleton Hospital -->\\n\")\n",
    "        \n",
    "        outf.write(\"</routes>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42421489-84dc-4eee-a96d-6aaec5c90150",
   "metadata": {},
   "source": [
    "## 4.3 Run the Simulation\n",
    "Once all traffic demand files have been generated, the simulation can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea985acd-2ec2-4516-a07c-1dddd8949a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMO_CONFIG_FILENAME = 'swansea.sumocfg'\n",
    "SWANSEA_POLY_FILENAME = 'swansea.poly.xml'\n",
    "TRIPINFO_OUTPUT_FILENAME = 'tripinfo_output.xml'\n",
    "FCD_OUTPUT_FILENAME = 'fcd_output.xml'\n",
    "STATS_OUTPUT_FILENAME = 'statistic_output.xml'\n",
    "SIMULATION_VIEW_FILENAME = 'simulation_view.xml'\n",
    "\n",
    "\n",
    "def generate_sim_view():\n",
    "    '''\n",
    "    Function to generate the GUI view settings file.\n",
    "    This file was really only necessary in testing using the GUI application.\n",
    "    \n",
    "    '''\n",
    "    with open(SIMULATION_VIEW_FILENAME, 'w') as outf:\n",
    "        outf.write(\"<viewsettings>\\n\")\n",
    "        outf.write(\"    <scheme name='real world'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    <delay value='0'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"</viewsettings>\")\n",
    "\n",
    "\n",
    "def generate_sumo_config():\n",
    "    '''\n",
    "    Function to generate the sumo config file for each simulation.\n",
    "    \n",
    "    '''\n",
    "    with open(SUMO_CONFIG_FILENAME, 'w') as outf:\n",
    "        outf.write(\"<configuration>\\n\")\n",
    "        \n",
    "        # input\n",
    "        outf.write(\"    <input>\\n\")\n",
    "        outf.write(f\"        <net-file value='{SWANSEA_NET_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(f\"        <route-files value='{EV_FILENAME},{TRAFFIC_ROUTES_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(f\"        <additional-files value='{SWANSEA_POLY_FILENAME},{CALIBRATOR_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <begin value='0'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <end value='3600'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <seed value='202'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </input>\\n\")\n",
    "        \n",
    "        # lane changing model\n",
    "        #outf.write(\"    <lane_changing>\\n\")\n",
    "        #outf.write(\"        <lateral-resolution value='3.2'/>\\n\".replace(\"'\",'\"'))\n",
    "        #outf.write(\"    </lane_changing>\\n\")\n",
    "        \n",
    "        # processing\n",
    "        outf.write(\"    <processing>\\n\")\n",
    "        outf.write(\"        <ignore-route-errors value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </processing>\\n\")\n",
    "        \n",
    "        \n",
    "        # devices\n",
    "        outf.write(\"    <devices>\\n\")\n",
    "        outf.write(\"        <device.bluelight.explicit value='ev_trip'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <device.fcd.explicit value='ev_trip'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <device.rerouting.explicit value='ev_trip'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <device.rerouting.period value='100'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </devices>\\n\")\n",
    "        \n",
    "        # report\n",
    "        outf.write(\"    <report>\\n\")\n",
    "        outf.write(\"        <verbose value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <duration-log.statistics value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"        <no-step-log value='true'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(f\"        <tripinfo-output value='{TRIPINFO_OUTPUT_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(f\"        <fcd-output value='{FCD_OUTPUT_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(f\"        <statistic-output value='{STATS_OUTPUT_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </report>\\n\") \n",
    "        \n",
    "        # gui settings\n",
    "        outf.write(\"    <gui_only>\\n\")\n",
    "        outf.write(f\"        <gui-settings-file value='{SIMULATION_VIEW_FILENAME}'/>\\n\".replace(\"'\",'\"'))\n",
    "        outf.write(\"    </gui_only>\\n\")        \n",
    "        \n",
    "        outf.write(\"</configuration>\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031b2fa-7a77-4732-b750-72d7f0655233",
   "metadata": {},
   "source": [
    "Need some way to handle the output files. This involves converting them from XML files to Pandas DataFrames. This may have to be done via conversion to CSV first, but Pandas do have a `read_xml` function that can read some flatter XML documents according to [the documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_xml.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6821061-6cff-47df-afcf-6c3c1b23e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripinfo_output_df_list = []\n",
    "ev_output_df_list = []\n",
    "fcd_output_df_list = []\n",
    "stats_output_df_list = []\n",
    "edge_output_df_list = []\n",
    "calibrator_output_df_list = []\n",
    "\n",
    "\n",
    "def import_tripinfo(model_name):\n",
    "    '''Function to import tripinfo output'''\n",
    "    tripinfo_output = pd.read_xml(TRIPINFO_OUTPUT_FILENAME)\n",
    "    cols = ['id','depart','arrival','duration','routeLength','waitingTime', 'timeLoss','vaporized']\n",
    "    tripinfo_output = tripinfo_output[cols]\n",
    "    tripinfo_output['model_name'] = model_name\n",
    "    \n",
    "    # extract the ev trip data\n",
    "    ev_output = tripinfo_output.loc[tripinfo_output['id']=='ev_trip']\n",
    "    ev_output_df_list.append(ev_output)\n",
    "    \n",
    "    # drop ev trip data\n",
    "    tripinfo_output = tripinfo_output.drop(ev_output.index[0]).reset_index(drop=True)\n",
    "    \n",
    "    tripinfo_output_df_list.append(tripinfo_output)\n",
    "    \n",
    "    \n",
    "def import_fcd(model_name):\n",
    "    '''Function to import fcd output'''\n",
    "    fcd_output = pd.read_xml(FCD_OUTPUT_FILENAME, xpath=\".//vehicle\")\n",
    "    fcd_output['model_name'] = model_name\n",
    "    fcd_output_df_list.append(fcd_output)\n",
    "        \n",
    "\n",
    "def import_stats(model_name):\n",
    "    '''Function to import stats output'''\n",
    "    stats_output = pd.read_xml(STATS_OUTPUT_FILENAME)\n",
    "    \n",
    "    # extract numeric values into a series\n",
    "    stats_dict = {}\n",
    "    for col in stats_output.columns:\n",
    "        stats_dict[col] = np.array(stats_output[col].dropna())[0]\n",
    "    \n",
    "    stats_dict['model_name'] = model_name    \n",
    "    stats_output_df_list.append(pd.Series(stats_dict))\n",
    "    \n",
    "    \n",
    "def import_edge(model_name):\n",
    "    '''Function to import edge output'''\n",
    "    edge_output = pd.read_xml(EDGE_OUTPUT_FILENAME, xpath=\".//edge\")\n",
    "    cols = ['id','sampledSeconds','traveltime','density','occupancy',\n",
    "            'waitingTime', 'timeLoss','speed', 'vaporized']\n",
    "    edge_output = edge_output[cols]\n",
    "    edge_output['model_name'] = model_name\n",
    "    edge_output_df_list.append(edge_output)\n",
    "    \n",
    "    \n",
    "def import_calibrator(model_name):\n",
    "    '''Function to import calibrator output'''\n",
    "    calibrator_output = pd.read_xml(CALIBRATOR_OUTPUT_FILENAME)\n",
    "    cols = ['id','nVehContrib','removed','inserted','cleared','flow',\n",
    "            'aspiredFlow', 'speed', 'aspiredSpeed']\n",
    "    calibrator_output = calibrator_output[cols]\n",
    "    calibrator_output['model_name'] = model_name\n",
    "    calibrator_output_df_list.append(calibrator_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661732d4-2aa9-4238-8837-f33c7cd628e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_simulations(model_names):\n",
    "    '''\n",
    "    Function to run the simulations for each model.\n",
    "    \n",
    "    '''\n",
    "    for model_name in model_names:\n",
    "        start = time.time()\n",
    "        \n",
    "        # generate xml files\n",
    "        generate_calibrator_file(model_name=model_name)\n",
    "        generate_ev_file()\n",
    "        generate_sim_view()\n",
    "        generate_sumo_config()  \n",
    "        print('Generated all xml files')\n",
    "        \n",
    "        # run simulation\n",
    "        print('Starting simulation...')\n",
    "        !cd C:\\Users\\ollir\\OneDrive\\Documents\\University\\Data Science MSc\\Dissertation\\To-Fly-or-Not-to-Fly\\simulation\n",
    "        !sumo -c {SUMO_CONFIG_FILENAME}\n",
    "        print(f'Generated files and ran simulation for {model_name} model in {np.round(time.time()-start, 2)} seconds')\n",
    "        \n",
    "        # convert output files to DataFrame and add to list\n",
    "        import_tripinfo(model_name)\n",
    "        import_fcd(model_name)\n",
    "        import_stats(model_name)\n",
    "        import_edge(model_name)\n",
    "        import_calibrator(model_name)\n",
    "\n",
    "\n",
    "model_names = ['Actual','LR', 'RF', 'CNN']\n",
    "run_simulations(model_names)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e580549-bc39-47c2-a66d-40f2faaa0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tripinfo_output_df_list\n",
    "%store ev_output_df_list\n",
    "%store fcd_output_df_list\n",
    "%store stats_output_df_list\n",
    "%store edge_output_df_list\n",
    "%store calibrator_output_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e029ba4-7aba-4cf4-b3c0-1efe0e83a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r tripinfo_output_df_list\n",
    "# %store -r ev_output_df_list\n",
    "# %store -r fcd_output_df_list\n",
    "# %store -r stats_output_df_list\n",
    "# %store -r edge_output_df_list\n",
    "# %store -r calibrator_output_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a81214-f9a9-4815-8f19-deb9cdb6bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single dataframes from all simulation runs\n",
    "tripinfo_output = pd.concat(tripinfo_output_df_list)\n",
    "ev_output = pd.concat(ev_output_df_list)\n",
    "fcd_output = pd.concat(fcd_output_df_list)\n",
    "stats_output = pd.concat(stats_output_df_list, axis=1)\n",
    "edge_output = pd.concat(edge_output_df_list)\n",
    "calibrator_output = pd.concat(calibrator_output_df_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578635a-de86-4522-82d7-7f90a4e7c061",
   "metadata": {},
   "source": [
    "# 5. Visualise Simulation Results\n",
    "In this section we visualise the simulation outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5440ba-430c-4319-a82a-ef8eceaa4af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.1 Trip Info\n",
    "First, we visualise the `tripinfo_output` of the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce922d-7e38-4cfe-bb2b-3880c39f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripinfo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6221fa-2c03-4ef9-b13b-67f4e980ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripinfo_output.groupby(['model_name']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd43da-eb95-4e2b-b8fb-75824bb9ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable max rows warning\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "def depart_arrive():\n",
    "    '''\n",
    "    Visual to show how many vehicles departing/arriving as the simulation progresses.\n",
    "    \n",
    "    '''\n",
    "    depart_arrive = alt.Chart(tripinfo_output).transform_fold(\n",
    "        ['depart','arrival'],\n",
    "        as_=['Action','Time']\n",
    "    ).mark_line(\n",
    "        opacity=1\n",
    "    ).encode(\n",
    "        alt.X('Time:O', bin=alt.Bin(maxbins=50), title='Time of Departure in Simulation'),\n",
    "        alt.Y('count()', stack=None),\n",
    "        alt.Color('Action:N')\n",
    "    ).properties(\n",
    "        width=200,\n",
    "        height=200\n",
    "    ).facet(\n",
    "        column='model_name:N',\n",
    "        title='Departure vs Arrival Counts'\n",
    "    )\n",
    "\n",
    "    return depart_arrive\n",
    "\n",
    "depart_arrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ef5af-e491-43d3-8142-ad7d06741bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes():\n",
    "    '''\n",
    "    Visual to show box plots of trip durations, routeLength, waitingTime, and timeLoss\n",
    "    for each simulation. \n",
    "    Note: these return the MEDIAN (more robust to outliers)\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    features=['duration','routeLength','waitingTime','timeLoss']\n",
    "    \n",
    "    boxes = alt.concat(*(\n",
    "        alt.Chart(tripinfo_output, title=feature.title()).mark_boxplot(\n",
    "            outliers=False\n",
    "        ).encode(\n",
    "            alt.X('model_name:N', title='Model Name'),\n",
    "            alt.Y(feature+':Q', title=feature.capitalize()),\n",
    "            alt.Color('model_name:N', legend=None)\n",
    "        ).properties(\n",
    "            width=125,\n",
    "            height=125\n",
    "        )\n",
    "        for feature in features\n",
    "      ), columns=4\n",
    "    ).properties(\n",
    "            title = 'Boxplots of Traffic Route Data'\n",
    "    )  \n",
    "    \n",
    "    return boxes\n",
    "\n",
    "\n",
    "boxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efbfe4-34cb-454c-836c-0c7d649eceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaporised():\n",
    "    '''\n",
    "    Visual to show how many vehicles were vaporised and why. \n",
    "    \n",
    "    '''\n",
    "    vaporised = alt.Chart(tripinfo_output).transform_filter(\n",
    "        alt.FieldOneOfPredicate(field='vaporized', oneOf=['calibrator', 'teleport'])\n",
    "    ).mark_line(\n",
    "        opacity=1\n",
    "    ).encode(\n",
    "        alt.X('depart:O', bin=alt.Bin(maxbins=50), title='Time of Departure in Simulation'),\n",
    "        alt.Y('count():Q'),\n",
    "        alt.Color('vaporized'),\n",
    "        tooltip=['count()']\n",
    "    ).properties(\n",
    "        width=200,\n",
    "        height=200\n",
    "    ).facet(\n",
    "        column='model_name:N',\n",
    "        title='Vaporised Vehicles'\n",
    "    )\n",
    "\n",
    "    return vaporised\n",
    "\n",
    "\n",
    "vaporised()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016527b-5195-4a77-ac39-554be3083689",
   "metadata": {},
   "source": [
    "## 5.2 Emergency Vehicles\n",
    "Visualise the emergency vehicles and their tripinfo and FCD output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b1b39-4da5-4f37-9d3d-aa1b5222dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1e31c-bb00-4d95-96a8-44004c0384f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_tripinfo():\n",
    "    '''Visual of the EV route information'''\n",
    "\n",
    "    tripinfo = alt.Chart(ev_output).mark_bar().encode(\n",
    "        alt.Y('model_name:N', title=None),\n",
    "        alt.X(alt.repeat('row'), type='quantitative'),\n",
    "        color='model_name:N',\n",
    "        tooltip=[\n",
    "            alt.Tooltip('model_name:N', title='Model Name'),\n",
    "            alt.Tooltip(alt.repeat('row'), type='quantitative')]\n",
    "    ).repeat(\n",
    "        row=['routeLength','duration','waitingTime','timeLoss']\n",
    "    ).properties(\n",
    "        title='EV Trip Information per Simulation'\n",
    "    )\n",
    "    \n",
    "    return tripinfo\n",
    "\n",
    "\n",
    "ev_tripinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe44a-32cf-4441-b0ad-c41ecabf4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xy_to_latlon(df):\n",
    "    '''\n",
    "    Function to convert SUMO x and y coordinates to longitude\n",
    "    and latitude.\n",
    "    \n",
    "    '''\n",
    "    points_lonlat = [net.convertXY2LonLat(x,y) for x,y in zip(df['x'], df['y'])]\n",
    "    df['latitude'] = [lat for lon, lat in points_lonlat]\n",
    "    df['longitude'] = [lon for lon, lat in points_lonlat]\n",
    "    return df\n",
    "    \n",
    "\n",
    "fcd_output = convert_xy_to_latlon(fcd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c31f2a-7829-4e52-a21a-b7bde400066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ev_routes_folium(df):\n",
    "    '''\n",
    "    Function to plot EV routes from the FCD output df.\n",
    "    \n",
    "    '''\n",
    "    # initiate map\n",
    "    swansea_coords = [51.6195955, -3.9459248]\n",
    "    m = folium.Map(location=swansea_coords, zoom_start=11)\n",
    "    \n",
    "    \n",
    "    # iterate over models\n",
    "    # route_colours = ['#808080','#69BE28','#3DB7E4',] # grey, green, blue\n",
    "    route_colours = ['green','red','blue','orange']\n",
    "    for idx, model in enumerate(model_names):\n",
    "        filtered_df = df.loc[df['model_name']==model]\n",
    "        route = [(lon, lat) for lon, lat in zip(filtered_df['latitude'],filtered_df['longitude'])]\n",
    "        folium.PolyLine(route, tooltip=model, color=route_colours[idx], opacity=1).add_to(m)\n",
    "        \n",
    "    \n",
    "    # origin marker\n",
    "    origin = (filtered_df.iloc[0]['latitude'],filtered_df.iloc[0]['longitude'])\n",
    "    folium.Marker(origin, popup='Morriston Hospital', \n",
    "                  icon=folium.Icon(color='green',\n",
    "                                   icon='h-square', prefix='fa')).add_to(m)\n",
    "    \n",
    "    # destination marker\n",
    "    destination = (filtered_df.iloc[-1]['latitude'],filtered_df.iloc[-1]['longitude'])\n",
    "    folium.Marker(destination, popup='Singleton Hospital', \n",
    "                  icon=folium.Icon(color='red',\n",
    "                                   icon='h-square', prefix='fa')).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "plot_ev_routes_folium(fcd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708dbd4-b095-47a6-8270-fe4c0c753651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ev_fcd(ev_df, fcd_df):\n",
    "    '''\n",
    "    Function to take the route of each EV from fcd_output, \n",
    "    and merge it into ev_output as a polyline geometry.\n",
    "    \n",
    "    '''\n",
    "    routes_coords_list = []\n",
    "    for model in model_names:\n",
    "        filtered_fcd = fcd_df.loc[fcd_df['model_name']==model]\n",
    "        route_coords = [(lon, lat) for lon, lat in zip(filtered_fcd['longitude'], filtered_fcd['latitude'])]\n",
    "        routes_coords_list.append(route_coords)\n",
    "        \n",
    "    ev_df['geometry'] = [LineString(route) for route in routes_coords_list]\n",
    "    merged_df = gpd.GeoDataFrame(ev_df)    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "ev_output = merge_ev_fcd(ev_output, fcd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a5c16-7126-43a3-b6c1-dfdf9742eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_geometries(df, edge_col_name, using='edge_id'):\n",
    "    '''\n",
    "    Function to add a column containing the edge geometry in a \n",
    "    GeoPandas geometry form. These can be obtained from Edge object, or Edge IDs.\n",
    "    This is done by looking up the edge shape from the SUMO net.\n",
    "    Then the (x,y) coordinates are converted to (lon,lat) and finally,\n",
    "    converted to GeoPandas LineStrings.  \n",
    "    \n",
    "    '''\n",
    "    lines_lonlat_list = []\n",
    "    for edge in df[edge_col_name]:\n",
    "        if using == 'edge_id':\n",
    "            lines_xy = net.getEdge(edge).getShape() # edge_id\n",
    "        elif using == 'edge_object':\n",
    "            lines_xy = edge.getShape() # edge_object\n",
    "        lines_lonlat_list.append([net.convertXY2LonLat(x,y) for x,y in lines_xy])\n",
    "    df['geometry'] = [LineString(line) for line in lines_lonlat_list]\n",
    "    df_gdf = gpd.GeoDataFrame(df)        \n",
    "    return df_gdf\n",
    "        \n",
    "\n",
    "def get_edge_df():\n",
    "    edge_objects = net.getEdges()\n",
    "    edge_df = pd.DataFrame({'edges': edge_objects})\n",
    "    edge_df = get_edge_geometries(edge_df, edge_col_name='edges', using='edge_object')\n",
    "    return edge_df\n",
    "\n",
    "\n",
    "edge_df = get_edge_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a67e18-43a2-40e5-99d3-af658f881b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3ede7-3c91-42f5-a00c-c1b1b97cdbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ev_routes_altair():\n",
    "    '''\n",
    "    Function to plot EV routes on a map in Altair.\n",
    "    \n",
    "    '''\n",
    "    roads = alt.Chart(edge_df['geometry']).mark_geoshape(\n",
    "        filled=False,\n",
    "        strokeWidth=1,\n",
    "        color='lightgrey'\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    ev_routes = alt.Chart(ev_output).mark_geoshape(\n",
    "        filled=False,\n",
    "        strokeWidth=2\n",
    "    ).encode(\n",
    "        alt.Color('model_name:N'),\n",
    "        tooltip=[alt.Tooltip('model_name:N', title='Model Name'),\n",
    "                 alt.Tooltip('duration:Q', title='Duration (s)'),\n",
    "                 alt.Tooltip('waitingTime:Q', title='Waiting Time (s)'),\n",
    "                 alt.Tooltip('timeLoss:Q', title='Time Loss (s)')]\n",
    "    )\n",
    "    \n",
    "    speed_pos = alt.Chart(fcd_output).mark_circle().encode(\n",
    "        alt.Longitude('longitude:Q', title='Longitude'),\n",
    "        alt.Latitude('latitude:Q', title='Latitude'),\n",
    "        color='speed:Q',\n",
    "        tooltip=['speed:Q']\n",
    "    ).properties(\n",
    "        width=200,\n",
    "        height=200\n",
    "    ).facet(\n",
    "        column='model_name:N'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return (roads + ev_routes) & speed_pos & tripinfo\n",
    "    \n",
    "\n",
    "plot_ev_routes_altair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f83d9-7c12-4feb-b686-982dd9721c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa29845-b8d3-4ed2-a0ca-eafb89986cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cps_altair():\n",
    "    '''\n",
    "    Map of CPs around Swansea\n",
    "    \n",
    "    '''\n",
    "    roads = alt.Chart(edge_df['geometry']).mark_geoshape(\n",
    "        filled=False,\n",
    "        strokeWidth=1,\n",
    "        color='lightgrey'\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    cps = alt.Chart(cp_df.drop(columns=['closest_edges'])).mark_circle().encode(\n",
    "        alt.Longitude('longitude:Q', title='Longitude'),\n",
    "        alt.Latitude('latitude:Q', title='Latitude'),\n",
    "        tooltip=[alt.Tooltip('count_point_id:N', title='Count Point ID')]\n",
    "    )\n",
    "    \n",
    "    return roads + cps\n",
    "\n",
    "\n",
    "plot_cps_altair()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762aaa9-f15d-4fb3-8c0b-05654b154722",
   "metadata": {},
   "source": [
    "## 5.3 Simulation Statistics\n",
    "Here we visualise simulation statistics. To be honest, this data may just be better presented in a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e35d1e-77f7-4465-889c-97b225b025eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de985abf-b7b4-48b5-8011-35ac877a4c7f",
   "metadata": {},
   "source": [
    "## 5.4 Edge Data\n",
    "Here the edge output data is visualised to give some location context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97c118-f8cd-46cd-949f-e95f6848431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_further_values(df):\n",
    "    '''\n",
    "    Function to derive further values from the edge output data\n",
    "    as suggested on the SUMO wiki page:\n",
    "    https://sumo.dlr.de/docs/Simulation/Output/Lane-_or_Edge-based_Traffic_Measures.html\n",
    "    \n",
    "    '''\n",
    "    df['avg_num_veh'] = df['sampledSeconds']/3600 # every hour\n",
    "    df['avg_vol'] = df['speed']*3.6*df['density']    \n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "edge_output = derive_further_values(edge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3243fe7-ca57-4915-acf3-8dc12361507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_output = get_edge_geometries(edge_output, edge_col_name='id', using='edge_id')\n",
    "edge_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4ec1f-e476-423a-9af6-06fcd31db9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_output.groupby(['model_name']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7b912-67dd-4003-abc0-9c2eae96c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(edge_output.drop(columns['geometry'])).mark_boxplot(\n",
    "    outliers=False\n",
    ").encode(\n",
    "    alt.Y('model_name:N', title='Model Name'),\n",
    "    alt.X('speed:Q', title='Speed (m/s)'),\n",
    "    alt.Color('model_name:N')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac3b42-3082-4c04-93db-38c5545f4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_output.loc[edge_output['model_name']=='CNN'].hist(column='speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140822f-9441-4e79-9dda-7ebd42af135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_map():\n",
    "    '''Visual of the edge map, coloured by some feature.'''\n",
    "      \n",
    "    edge_map = alt.concat(*(\n",
    "        alt.Chart(edge_output.loc[edge_output['model_name']==name], title=name).mark_geoshape(\n",
    "            filled=False,\n",
    "            strokeWidth=1\n",
    "        ).encode(\n",
    "            alt.Color('speed:Q', scale=alt.Scale(scheme='yellowgreenblue')),#, bin=alt.Bin(maxbins=6)),\n",
    "            tooltip=['id','speed']\n",
    "        ).properties(\n",
    "            width=600,\n",
    "            height=400\n",
    "        )\n",
    "        for name in model_names\n",
    "      ), columns=1\n",
    "    ).properties(\n",
    "        title = 'Mean Edge Speed of Simulations'\n",
    "    )  \n",
    "\n",
    "    return edge_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e22eb4-6eda-4c2c-ac3d-d801bf101495",
   "metadata": {},
   "source": [
    "## 5.5 Calibrator Data\n",
    "In this section we visualise the calibrator output data for each simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20319548-8907-4c03-a41c-dd9421c84398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges_to_calibrator_file():\n",
    "    '''\n",
    "    Function to add edges to the calibrator output file.\n",
    "    \n",
    "    '''\n",
    "    calibrator_edges = pd.read_xml(CALIBRATOR_FILENAME, xpath=\".//calibrator\")\n",
    "    return pd.merge(calibrator_output, calibrator_edges[['id','edge']], on = 'id', how = 'inner')\n",
    "\n",
    "\n",
    "calibrator_output = add_edges_to_calibrator_file()\n",
    "calibrator_output = get_edge_geometries(calibrator_output, edge_col_name='edge', using='edge_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c5079-b711-4960-b556-ad6fe2493403",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b762a87-9096-455f-a21f-a0f997965035",
   "metadata": {},
   "source": [
    "## 5.6 Simulation Results DataFrame\n",
    "Just a table of simulation numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7e153-d2d8-472e-86dc-9f2cfc9517c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d194bacf-ba13-4fc5-84d0-f0837fb4109f",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "\n",
    "Just some things to revist and improve:\n",
    "\n",
    "#### Simulation wise\n",
    "- Double check that the removal of certain vehicle classes has been set correctly for the calibrators. The simulation will need to be re-run to do this. \n",
    "    - [This forum post](https://www.eclipse.org/lists/sumo-user/msg06014.html) was useful, as well as [the Calibrator page](https://sumo.dlr.de/docs/Simulation/Calibrator.html) on this SUMO wiki which stated `the normal behavior is to replace the type of the passing vehicles with the type set in the flow element.`\n",
    "- Try and get the sublane model to work. This is set by putting `<lateral-resolution value=\"3.2\"/>` in the `sumocfg` file. Take a look at the [Sublane Model wiki page](https://sumo.dlr.de/docs/Simulation/SublaneModel.html) for more details about specific parameters that can be set.\n",
    "    - Can look at [this vType deafault parameter page](https://sumo.dlr.de/docs/Vehicle_Type_Parameter_Defaults.html) for vehicle widths.\n",
    "- Also worth exploring how to disregard right-of-way and traffic lights. As per the [Emergency Vehicle simulation page](https://sumo.dlr.de/docs/Simulation/Emergency.html) this may have to be done through `TraCI speed mode command` but unsure if that is a feasible solution for my goal.\n",
    "\n",
    "#### Writing wise\n",
    "- During search for answers regarding sublane model, came across multiple papers by `Laura Bieker-Walz` from the German Aerospace Center that focused on simulating EVs in SUMO. [The obrusnik paper](https://wiki.control.fel.cvut.cz/mediawiki/images/0/00/Dp_2019_obrusnik_vit.pdf) mentioned a number of sources in the `EV modelling` section. Two papers I came across can be found [here](https://elib.dlr.de/118034/1/SUMO2017_bieker_emergency_vehicle_final.pdf) and [here](file:///C:/Users/ollir/Downloads/Analysis_of_the_traffic_behavior_of_emergency_vehicles_in_a_microscopic_traffic_simulation.pdf).\n",
    "\n",
    "#### Visualisation wise\n",
    "- All of it. Refer to the other jupyter notebook for a starting point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76eaa3-232e-4ac9-a13f-cfb86c4f4b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
